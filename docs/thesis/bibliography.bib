
@article{arcos-garcia_evaluation_2018,
	title = {Evaluation of deep neural networks for traffic sign detection systems},
	volume = {316},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S092523121830924X},
	doi = {10.1016/j.neucom.2018.08.009},
	abstract = {Traffic sign detection systems constitute a key component in trending real-world applications, such as autonomous driving, and driver safety and assistance. This paper analyses the state-of-the-art of several object-detection systems (Faster R-{CNN}, R-{FCN}, {SSD}, and {YOLO} V2) combined with various feature extractors (Resnet V1 50, Resnet V1 101, Inception V2, Inception Resnet V2, Mobilenet V1, and Darknet-19) previously developed by their corresponding authors. We aim to explore the properties of these object-detection models which are modified and specifically adapted to the traffic sign detection problem domain by means of transfer learning. In particular, various publicly available object-detection models that were pre-trained on the Microsoft {COCO} dataset are fine-tuned on the German Traffic Sign Detection Benchmark dataset. The evaluation and comparison of these models include key metrics, such as the mean average precision ({mAP}), memory allocation, running time, number of floating point operations, number of parameters of the model, and the effect of traffic sign image sizes. Our findings show that Faster R-{CNN} Inception Resnet V2 obtains the best {mAP}, while R-{FCN} Resnet 101 strikes the best trade-off between accuracy and execution time. {YOLO} V2 and {SSD} Mobilenet merit a special mention, in that the former achieves competitive accuracy results and is the second fastest detector, while the latter, is the fastest and the lightest model in terms of memory consumption, making it an optimal choice for deployment in mobile and embedded devices.},
	pages = {332--344},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Arcos-García, Álvaro and Álvarez-García, Juan A. and Soria-Morillo, Luis M.},
	urldate = {2019-10-10},
	date = {2018-11-17},
	keywords = {Convolutional neural network, Deep learning, Traffic sign detection},
	file = {ScienceDirect Snapshot:/home/ritzo/Zotero/storage/IUAY8K58/S092523121830924X.html:text/html}
}

@article{goodarzi_comparison_nodate,
	title = {Comparison and Optimization of {CNN}-based Object Detectors for Fisheye Cameras},
	pages = {132},
	author = {Goodarzi, Payam},
	langid = {english},
	file = {Goodarzi - Comparison and Optimization of CNN-based Object De.pdf:/home/ritzo/Zotero/storage/DEPRDRID/Goodarzi - Comparison and Optimization of CNN-based Object De.pdf:application/pdf}
}

@article{dai_deformable_2017,
	title = {Deformable Convolutional Networks},
	url = {http://arxiv.org/abs/1703.06211},
	abstract = {Convolutional neural networks ({CNNs}) are inherently limited to model geometric transformations due to the ﬁxed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of {CNNs}, namely, deformable convolution and deformable {RoI} pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing {CNNs} and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the ﬁrst time, we show that learning dense spatial transformation in deep {CNNs} is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/ msracver/Deformable-{ConvNets}.},
	journaltitle = {{arXiv}:1703.06211 [cs]},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	urldate = {2019-09-26},
	date = {2017-03-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.06211},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Dai et al. - 2017 - Deformable Convolutional Networks.pdf:/home/ritzo/Zotero/storage/C7SBZNDD/Dai et al. - 2017 - Deformable Convolutional Networks.pdf:application/pdf}
}

@article{wu_recent_2019,
	title = {Recent Advances in Deep Learning for Object Detection},
	url = {http://arxiv.org/abs/1908.03673},
	abstract = {Object detection is a fundamental visual recognition problem in computer vision and has been widely studied in the past decades. Visual object detection aims to ﬁnd objects of certain target classes with precise localization in a given image and assign each object instance a corresponding class label. Due to the tremendous successes of deep learning based image classiﬁcation, object detection techniques using deep learning have been actively studied in recent years. In this paper, we give a comprehensive survey of recent advances in visual object detection with deep learning. By reviewing a large body of recent related work in literature, we systematically analyze the existing object detection frameworks and organize the survey into three major parts: (i) detection components, (ii) learning strategies, and (iii) applications \& benchmarks. In the survey, we cover a variety of factors affecting the detection performance in detail, such as detector architectures, feature learning, proposal generation, sampling strategies, etc. Finally, we discuss several future directions to facilitate and spur future research for visual object detection with deep learning.},
	journaltitle = {{arXiv}:1908.03673 [cs]},
	author = {Wu, Xiongwei and Sahoo, Doyen and Hoi, Steven C. H.},
	urldate = {2019-09-25},
	date = {2019-08-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1908.03673},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia},
	file = {Wu et al. - 2019 - Recent Advances in Deep Learning for Object Detect.pdf:/home/ritzo/Zotero/storage/V7HFYHK6/Wu et al. - 2019 - Recent Advances in Deep Learning for Object Detect.pdf:application/pdf}
}

@article{abadi_tensorflow:_nodate,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	pages = {19},
	author = {Abadi, Martın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	langid = {english},
	file = {Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:/home/ritzo/Zotero/storage/YRDIV8R7/Abadi et al. - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf}
}

@article{chen_simpledet:_2019,
	title = {{SimpleDet}: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition},
	url = {http://arxiv.org/abs/1903.05831},
	shorttitle = {{SimpleDet}},
	abstract = {Object detection and instance recognition play a central role in many {AI} applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an eﬃcient and open source object detection framework called {SimpleDet} which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. {SimpleDet} supports up-to-date detection models with best practice. {SimpleDet} also supports distributed training with nearlinear scaling out of box. Codes, examples and documents of {SimpleDet} can be found at https://github.com/tusimple/simpledet.},
	journaltitle = {{arXiv}:1903.05831 [cs]},
	author = {Chen, Yuntao and Han, Chenxia and Li, Yanghao and Huang, Zehao and Jiang, Yi and Wang, Naiyan and Zhang, Zhaoxiang},
	urldate = {2019-09-25},
	date = {2019-03-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.05831},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Chen et al. - 2019 - SimpleDet A Simple and Versatile Distributed Fram.pdf:/home/ritzo/Zotero/storage/LILYU66G/Chen et al. - 2019 - SimpleDet A Simple and Versatile Distributed Fram.pdf:application/pdf}
}

@article{li_scale-aware_2019,
	title = {Scale-Aware Trident Networks for Object Detection},
	url = {http://arxiv.org/abs/1901.01892},
	abstract = {Scale variation is one of the key challenges in object detection. In this work, we ﬁrst present a controlled experiment to investigate the effect of receptive ﬁelds for scale variation in object detection. Based on the ﬁndings from the exploration experiments, we propose a novel Trident Network ({TridentNet}) aiming to generate scale-speciﬁc feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive ﬁelds. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of {TridentNet} could achieve signiﬁcant improvements without any additional parameters and computational cost compared with the vanilla detector. On the {COCO} dataset, our {TridentNet} with {ResNet}-101 backbone achieves state-of-the-art single-model results of 48.4 {mAP}. Codes are available at https://git.io/fj5vR.},
	journaltitle = {{arXiv}:1901.01892 [cs]},
	author = {Li, Yanghao and Chen, Yuntao and Wang, Naiyan and Zhang, Zhaoxiang},
	urldate = {2019-09-25},
	date = {2019-01-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.01892},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2019 - Scale-Aware Trident Networks for Object Detection.pdf:/home/ritzo/Zotero/storage/R232MHB8/Li et al. - 2019 - Scale-Aware Trident Networks for Object Detection.pdf:application/pdf}
}

@article{cai_cascade_2017,
	title = {Cascade R-{CNN}: Delving into High Quality Object Detection},
	url = {http://arxiv.org/abs/1712.00726},
	shorttitle = {Cascade R-{CNN}},
	abstract = {In object detection, an intersection over union ({IoU}) threshold is required to deﬁne positives and negatives. An object detector, trained with low {IoU} threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the {IoU} thresholds. Two main factors are responsible for this: 1) overﬁtting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the {IoUs} for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-{CNN}, is proposed to address these problems. It consists of a sequence of detectors trained with increasing {IoU} thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overﬁtting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-{CNN} is shown to surpass all single-model object detectors on the challenging {COCO} dataset. Experiments also show that the Cascade R-{CNN} is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at https://github.com/zhaoweicai/cascade-rcnn.},
	journaltitle = {{arXiv}:1712.00726 [cs]},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	urldate = {2019-09-25},
	date = {2017-12-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.00726},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Cai and Vasconcelos - 2017 - Cascade R-CNN Delving into High Quality Object De.pdf:/home/ritzo/Zotero/storage/DPEK4PFK/Cai and Vasconcelos - 2017 - Cascade R-CNN Delving into High Quality Object De.pdf:application/pdf}
}

@article{peng_megdet:_2017,
	title = {{MegDet}: A Large Mini-Batch Object Detector},
	url = {http://arxiv.org/abs/1711.07240},
	shorttitle = {{MegDet}},
	abstract = {The development of object detection in the era of deep learning, from R-{CNN} [11], Fast/Faster R-{CNN} [10, 31] to recent Mask R-{CNN} [14] and {RetinaNet} [24], mainly come from novel network, new framework, or loss design. However, mini-batch size, a key factor for the training of deep neural networks, has not been well studied for object detection. In this paper, we propose a Large Mini-Batch Object Detector ({MegDet}) to enable the training with a large minibatch size up to 256, so that we can effectively utilize at most 128 {GPUs} to signiﬁcantly shorten the training time. Technically, we suggest a warmup learning rate policy and Cross-{GPU} Batch Normalization, which together allow us to successfully train a large mini-batch detector in much less time (e.g., from 33 hours to 4 hours), and achieve even better accuracy. The {MegDet} is the backbone of our submission ({mmAP} 52.5\%) to {COCO} 2017 Challenge, where we won the 1st place of Detection task.},
	journaltitle = {{arXiv}:1711.07240 [cs]},
	author = {Peng, Chao and Xiao, Tete and Li, Zeming and Jiang, Yuning and Zhang, Xiangyu and Jia, Kai and Yu, Gang and Sun, Jian},
	urldate = {2019-09-24},
	date = {2017-11-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.07240},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Peng et al. - 2017 - MegDet A Large Mini-Batch Object Detector.pdf:/home/ritzo/Zotero/storage/FEQGVYZP/Peng et al. - 2017 - MegDet A Large Mini-Batch Object Detector.pdf:application/pdf}
}

@article{liu_cbnet:_2019,
	title = {{CBNet}: A Novel Composite Backbone Network Architecture for Object Detection},
	url = {http://arxiv.org/abs/1909.03625},
	shorttitle = {{CBNet}},
	abstract = {In existing {CNN} based detectors, the backbone network is a very important component for basic feature1 extraction, and the performance of the detectors highly depends on it. In this paper, we aim to achieve better detection performance by building a more powerful backbone from existing backbones like {ResNet} and {ResNeXt}. Speciﬁcally, we propose a novel strategy for assembling multiple identical backbones by composite connections between the adjacent backbones, to form a more powerful backbone named Composite Backbone Network ({CBNet}). In this way, {CBNet} iteratively feeds the output features of the previous backbone, namely high-level features, as part of input features to the succeeding backbone, in a stage-by-stage fashion, and ﬁnally the feature maps of the last backbone (named Lead Backbone) are used for object detection. We show that {CBNet} can be very easily integrated into most state-of-the-art detectors and signiﬁcantly improve their performances. For example, it boosts the {mAP} of {FPN}, Mask R-{CNN} and Cascade R-{CNN} on the {COCO} dataset by about 1.5 to 3.0 percent. Meanwhile, experimental results show that the instance segmentation results can also be improved. Specially, by simply integrating the proposed {CBNet} into the baseline detector Cascade Mask R-{CNN}, we achieve a new state-of-the-art result on {COCO} dataset ({mAP} of 53.3) with single model, which demonstrates great effectiveness of the proposed {CBNet} architecture. Code will be made available on https://github.com/{PKUbahuangliuhe}/{CBNet}.},
	journaltitle = {{arXiv}:1909.03625 [cs]},
	author = {Liu, Yudong and Wang, Yongtao and Wang, Siwei and Liang, {TingTing} and Zhao, Qijie and Tang, Zhi and Ling, Haibin},
	urldate = {2019-09-24},
	date = {2019-09-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.03625},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Ausblick, Bachelor},
	file = {Liu et al. - 2019 - CBNet A Novel Composite Backbone Network Architec.pdf:/home/ritzo/Zotero/storage/VPFE9EJD/Liu et al. - 2019 - CBNet A Novel Composite Backbone Network Architec.pdf:application/pdf}
}

@article{xie_aggregated_2016,
	title = {Aggregated Residual Transformations for Deep Neural Networks},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the {ImageNet}-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named {ResNeXt}, are the foundations of our entry to the {ILSVRC} 2016 classification task in which we secured 2nd place. We further investigate {ResNeXt} on an {ImageNet}-5K set and the {COCO} detection set, also showing better results than its {ResNet} counterpart. The code and models are publicly available online.},
	journaltitle = {{arXiv}:1611.05431 [cs]},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	urldate = {2019-09-24},
	date = {2016-11-16},
	eprinttype = {arxiv},
	eprint = {1611.05431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.05431 PDF:/home/ritzo/Zotero/storage/H6X8QBW2/Xie et al. - 2016 - Aggregated Residual Transformations for Deep Neura.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/4LYE9CWZ/1611.html:text/html}
}

@article{razakarivony_vehicle_2016,
	title = {Vehicle detection in aerial imagery : A small target detection benchmark},
	volume = {34},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320315002187},
	doi = {10.1016/j.jvcir.2015.11.002},
	shorttitle = {Vehicle detection in aerial imagery},
	abstract = {This paper introduces {VEDAI}: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit diﬀerent variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by diﬀerent people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for diﬀerent settings of these algorithms, to illustrate the diﬃculties of the task and provide baseline comparisons.},
	pages = {187--203},
	journaltitle = {Journal of Visual Communication and Image Representation},
	shortjournal = {Journal of Visual Communication and Image Representation},
	author = {Razakarivony, Sebastien and Jurie, Frederic},
	urldate = {2019-09-23},
	date = {2016-01},
	langid = {english},
	keywords = {Bachelor},
	file = {Razakarivony and Jurie - 2016 - Vehicle detection in aerial imagery  A small targ.pdf:/home/ritzo/Zotero/storage/8SMZG3HW/Razakarivony and Jurie - 2016 - Vehicle detection in aerial imagery  A small targ.pdf:application/pdf}
}

@article{huang_speed/accuracy_2016,
	title = {Speed/accuracy trade-offs for modern convolutional object detectors},
	url = {http://arxiv.org/abs/1611.10012},
	abstract = {The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difﬁcult due to different base feature extractors (e.g., {VGG}, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a uniﬁed implementation of the Faster R-{CNN} [31], R-{FCN} [6] and {SSD} [26] systems, which we view as “meta-architectures” and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the {COCO} detection task.},
	journaltitle = {{arXiv}:1611.10012 [cs]},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	urldate = {2019-09-17},
	date = {2016-11-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1611.10012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Bachelor},
	file = {1611.10012.pdf:/home/moritz/Downloads/1611.10012.pdf:application/pdf}
}

@article{goodfellow_generative_nodate,
	title = {Generative Adversarial Nets},
	pages = {9},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	langid = {english},
	file = {Goodfellow et al. - Generative Adversarial Nets.pdf:/home/ritzo/Zotero/storage/PBZ7AJMH/Goodfellow et al. - Generative Adversarial Nets.pdf:application/pdf}
}

@article{li_lecture_nodate,
	title = {Lecture 11: Detection and Segmentation},
	pages = {95},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	langid = {english},
	file = {Li et al. - Lecture 11 Detection and Segmentation.pdf:/home/ritzo/Zotero/storage/CTPGH39U/Li et al. - Lecture 11 Detection and Segmentation.pdf:application/pdf}
}

@article{wei_network_nodate,
	title = {Network Morphism},
	abstract = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We deﬁne this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The ﬁrst requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we ﬁrst introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
	pages = {9},
	author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
	langid = {english},
	file = {Wei et al. - Network Morphism.pdf:/home/ritzo/Zotero/storage/ALUQXH5L/Wei et al. - Network Morphism.pdf:application/pdf}
}

@article{chen_net2net:_2015,
	title = {Net2Net: Accelerating Learning via Knowledge Transfer},
	url = {http://arxiv.org/abs/1511.05641},
	shorttitle = {Net2Net},
	abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a signiﬁcantly larger neural net. During real-world workﬂows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of functionpreserving transformations between neural network speciﬁcations. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the {ImageNet} dataset.},
	journaltitle = {{arXiv}:1511.05641 [cs]},
	author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
	urldate = {2019-09-24},
	date = {2015-11-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.05641},
	keywords = {Computer Science - Machine Learning},
	file = {Chen et al. - 2015 - Net2Net Accelerating Learning via Knowledge Trans.pdf:/home/ritzo/Zotero/storage/L7KYITTR/Chen et al. - 2015 - Net2Net Accelerating Learning via Knowledge Trans.pdf:application/pdf}
}

@article{kragh_fieldsafe:_2017,
	title = {{FieldSAFE}: Dataset for Obstacle Detection in Agriculture},
	volume = {17},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/17/11/2579},
	doi = {10.3390/s17112579},
	shorttitle = {{FieldSAFE}},
	abstract = {In this paper, we present a multi-modal dataset for obstacle detection in agriculture. The dataset comprises approximately 2 h of raw sensor data from a tractor-mounted sensor system in a grass mowing scenario in Denmark, October 2016. Sensing modalities include stereo camera, thermal camera, web camera, 360◦ camera, {LiDAR} and radar, while precise localization is available from fused {IMU} and {GNSS}. Both static and moving obstacles are present, including humans, mannequin dolls, rocks, barrels, buildings, vehicles and vegetation. All obstacles have ground truth object labels and geographic coordinates.},
	pages = {2579},
	number = {11},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Kragh, Mikkel and Christiansen, Peter and Laursen, Morten and Larsen, Morten and Steen, Kim and Green, Ole and Karstoft, Henrik and Jørgensen, Rasmus},
	urldate = {2019-09-20},
	date = {2017-11-09},
	langid = {english},
	file = {sensors-17-02579 (1).pdf:/home/moritz/Downloads/Telegram Desktop/sensors-17-02579 (1).pdf:application/pdf}
}

@article{higgs_protractor:_nodate,
	title = {{ProTractor}: A Lightweight Ground Imaging and Analysis System for Early-Season Field Phenotyping},
	abstract = {Acquiring high-resolution images in the ﬁeld for imagebased crop phenotyping is typically performed by complicated, custom built “pheno-mobiles.” In this paper, we demonstrate that large datasets of crop row images can be easily acquired with consumer cameras attached to a regular tractor. Localization and labeling of individual rows of plants are performed by a computer vision approach, rather than sophisticated real-time geolocation hardware on the tractor. We evaluate our approach for cropping rows of early-season plants from a Brassica carinata ﬁeld trial where we achieve 100\% recall and 99\% precision. We also demonstrate a proof-of-concept plant counting method for our {ProTractor} system using an object detection network that achieves a mean average precision of 0.82 when detecting plants, and an R2 of 0.89 when counting plants. The {ProTractor} design and software are open source to advance the collection of large outdoor plant phenotyping datasets with inexpensive and easy to use acquisition systems.},
	pages = {10},
	author = {Higgs, Nico and Leyeza, Blanche and Ubbens, Jordan and Kocur, Josh},
	langid = {english},
	file = {Higgs_ProTractor_A_Lightweight_G.pdf:/home/moritz/Downloads/Telegram Desktop/Higgs_ProTractor_A_Lightweight_G.pdf:application/pdf}
}

@book{moses_imageai_2018,
	title = {{ImageAI}, an open source python library built to empower developers to build applications and systems with self-contained Computer Vision capabilities},
	url = {https://github.com/OlafenwaMoses/ImageAI},
	author = {{Moses} and Olafenwa, John},
	date = {2018-03}
}

@inproceedings{jung_resnet-based_2017,
	title = {{ResNet}-Based Vehicle Classification and Localization in Traffic Surveillance Systems},
	booktitle = {The {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR}) Workshops},
	author = {Jung, Heechul and Choi, Min-Kook and Jung, Jihun and Lee, Jin-Hee and Kwon, Soon and Young Jung, Woo},
	date = {2017-07},
	file = {Jung_ResNet-Based_Vehicle_Classification_CVPR_2017_paper.pdf:/home/moritz/Downloads/Jung_ResNet-Based_Vehicle_Classification_CVPR_2017_paper.pdf:application/pdf}
}

@article{ren_faster_2015,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	url = {http://arxiv.org/abs/1506.01497},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} [1] and Fast R-{CNN} [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network ({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The {RPN} is trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. We further merge {RPN} and Fast R-{CNN} into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the {RPN} component tells the uniﬁed network where to look. For the very deep {VGG}-16 model [3], our detection system has a frame rate of 5fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007, 2012, and {MS} {COCO} datasets with only 300 proposals per image. In {ILSVRC} and {COCO} 2015 competitions, Faster R-{CNN} and {RPN} are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	journaltitle = {{arXiv}:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	urldate = {2019-09-17},
	date = {2015-06-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1506.01497.pdf:/home/moritz/Downloads/1506.01497.pdf:application/pdf}
}

@article{liu_ssd:_2016,
	title = {{SSD}: Single Shot {MultiBox} Detector},
	volume = {9905},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	shorttitle = {{SSD}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named {SSD}, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. {SSD} is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes {SSD} easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the {PASCAL} {VOC}, {COCO}, and {ILSVRC} datasets conﬁrm that {SSD} has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, {SSD} achieves 74.3\% {mAP}1 on {VOC}2007 test at 59 {FPS} on a Nvidia Titan X and for 512 × 512 input, {SSD} achieves 76.9\% {mAP}, outperforming a comparable state-of-the-art Faster R-{CNN} model. Compared to other single stage methods, {SSD} has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	pages = {21--37},
	journaltitle = {{arXiv}:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	urldate = {2019-09-17},
	date = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1512.02325.pdf:/home/moritz/Downloads/1512.02325.pdf:application/pdf}
}

@article{dai_r-fcn:_2016,
	title = {R-{FCN}: Object Detection via Region-based Fully Convolutional Networks},
	url = {http://arxiv.org/abs/1605.06409},
	shorttitle = {R-{FCN}},
	abstract = {We present region-based, fully convolutional networks for accurate and efﬁcient object detection. In contrast to previous region-based detectors such as Fast/Faster R-{CNN} [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classiﬁcation and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classiﬁer backbones, such as the latest Residual Networks ({ResNets}) [9], for object detection. We show competitive results on the {PASCAL} {VOC} datasets (e.g., 83.6\% {mAP} on the 2007 set) with the 101-layer {ResNet}. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-{CNN} counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
	journaltitle = {{arXiv}:1605.06409 [cs]},
	author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
	urldate = {2019-09-17},
	date = {2016-05-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.06409},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1605.06409.pdf:/home/moritz/Downloads/1605.06409.pdf:application/pdf}
}

@article{redmon_yolo9000:_2016,
	title = {{YOLO}9000: Better, Faster, Stronger},
	url = {http://arxiv.org/abs/1612.08242},
	shorttitle = {{YOLO}9000},
	abstract = {We introduce {YOLO}9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the {YOLO} detection method, both novel and drawn from prior work. The improved model, {YOLOv}2, is state-of-the-art on standard detection tasks like {PASCAL} {VOC} and {COCO}. Using a novel, multi-scale training method the same {YOLOv}2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 {FPS}, {YOLOv}2 gets 76.8 {mAP} on {VOC} 2007. At 40 {FPS}, {YOLOv}2 gets 78.6 {mAP}, outperforming state-of-the-art methods like Faster {RCNN} with {ResNet} and {SSD} while still running signiﬁcantly faster. Finally we propose a method to jointly train on object detection and classiﬁcation. Using this method we train {YOLO}9000 simultaneously on the {COCO} detection dataset and the {ImageNet} classiﬁcation dataset. Our joint training allows {YOLO}9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the {ImageNet} detection task. {YOLO}9000 gets 19.7 {mAP} on the {ImageNet} detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in {COCO}, {YOLO}9000 gets 16.0 {mAP}. But {YOLO} can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	journaltitle = {{arXiv}:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	urldate = {2019-09-17},
	date = {2016-12-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1612.08242.pdf:/home/moritz/Downloads/1612.08242.pdf:application/pdf}
}

@article{lin_feature_2016,
	title = {Feature Pyramid Networks for Object Detection},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network ({FPN}), shows signiﬁcant improvement as a generic feature extractor in several applications. Using {FPN} in a basic Faster R-{CNN} system, our method achieves state-of-the-art singlemodel results on the {COCO} detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the {COCO} 2016 challenge winners. In addition, our method can run at 6 {FPS} on a {GPU} and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	journaltitle = {{arXiv}:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	urldate = {2019-09-17},
	date = {2016-12-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.03144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1612.03144.pdf:/home/moritz/Downloads/1612.03144.pdf:application/pdf;arXiv\:1612.03144 PDF:/home/ritzo/Zotero/storage/A63TJLRU/Lin et al. - 2016 - Feature Pyramid Networks for Object Detection.pdf:application/pdf}
}

@article{lin_focal_2017,
	title = {Focal Loss for Dense Object Detection},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-{CNN}, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call {RetinaNet}. Our results show that when trained with the focal loss, {RetinaNet} is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	journaltitle = {{arXiv}:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2019-09-17},
	date = {2017-08-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1708.02002},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1708.02002.pdf:/home/moritz/Downloads/1708.02002.pdf:application/pdf}
}

@article{redmon_you_2015,
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	url = {http://arxiv.org/abs/1506.02640},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	journaltitle = {{arXiv}:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2019-09-17},
	date = {2015-06-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1506.02640.pdf:/home/moritz/Downloads/1506.02640.pdf:application/pdf}
}

@article{girshick_region-based_2016,
	title = {Region-Based Convolutional Networks for Accurate Object Detection and Segmentation},
	volume = {38},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7112511/},
	doi = {10.1109/TPAMI.2015.2437384},
	abstract = {Object detection performance, as measured on the canonical {PASCAL} {VOC} Challenge datasets, plateaued in the ﬁnal years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision ({mAP}) by more than 50\% relative to the previous best result on {VOC} 2012—achieving a {mAP} of 62.4\%. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks ({CNNs}) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, boosts performance signiﬁcantly. Since we combine region proposals with {CNNs}, we call the resulting model an R-{CNN} or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/∼rbg/rcnn.},
	pages = {142--158},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	urldate = {2019-09-17},
	date = {2016-01-01},
	langid = {english},
	file = {rcnn_pami.pdf:/home/moritz/Downloads/rcnn_pami.pdf:application/pdf}
}

@article{uijlings_selective_2013,
	title = {Selective Search for Object Recognition},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99\% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1.},
	pages = {154--171},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	urldate = {2019-09-17},
	date = {2013-09},
	langid = {english},
	file = {selectiveSearchDraft.pdf:/home/moritz/Downloads/selectiveSearchDraft.pdf:application/pdf}
}

@article{girshick_fast_2015,
	title = {Fast R-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-{CNN}) for object detection. Fast R-{CNN} builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-{CNN} employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-{CNN} trains the very deep {VGG}16 network 9× faster than R-{CNN}, is 213× faster at test-time, and achieves a higher {mAP} on {PASCAL} {VOC} 2012. Compared to {SPPnet}, Fast R-{CNN} trains {VGG}16 3× faster, tests 10× faster, and is more accurate. Fast R-{CNN} is implemented in Python and C++ (using Caffe) and is available under the open-source {MIT} License at https: //github.com/rbgirshick/fast-rcnn.},
	journaltitle = {{arXiv}:1504.08083 [cs]},
	author = {Girshick, Ross},
	urldate = {2019-09-17},
	date = {2015-04-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1504.08083.pdf:/home/moritz/Downloads/1504.08083.pdf:application/pdf}
}

@article{li_lecture_nodate-1,
	title = {Lecture 5: Convolutional Neural Networks},
	pages = {78},
	author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
	langid = {english},
	file = {Li et al. - Lecture 5 Convolutional Neural Networks.pdf:/home/ritzo/Zotero/storage/4Y6DBNQF/Li et al. - Lecture 5 Convolutional Neural Networks.pdf:application/pdf}
}

@article{wang_study_2017,
	title = {A Study on Camera Array and Its Applications**This research is funded by the State Key Laboratory of Geo-information Engineering under grant agreement {NO}. {SKLGIE}2015-M-3-4 and is supported by National Science Foundation of China (Grant No. 61473230), National Science Foundation for Young Scholars of China (Grant No. 61603303) and Aviation Science Foundation(Grant No. 2014ZC53030).},
	volume = {50},
	issn = {2405-8963},
	url = {http://www.sciencedirect.com/science/article/pii/S2405896317322681},
	doi = {10.1016/j.ifacol.2017.08.1662},
	series = {20th {IFAC} World Congress},
	abstract = {The reduced cost of cameras and the complex scenes make it possible and necessary to replace the monocular camera with camera array under certain situations. In this paper, we first provide a review on the existing camera arrays and sort them according to the array arrangement, and then give an overview of the imaging properties that are benefited from the camera array including dynamic range, resolution, seeing through occlusions and depth estimation. At last, a novel camera array-based airborne optical system is proposed to meet the intelligence, surveillance and reconnaissance ({ISR}) requirements for large field of view ({FOV}), high dynamic range and resolution, multi-view and multiple dimensions imaging. For the sake of onboard application of this system in practice, some key technologies are highlighted that need to be developed in the future research, such as the self-calibration and the synthetic aperture imaging by camera arrays on mobile platforms.},
	pages = {10323--10328},
	number = {1},
	journaltitle = {{IFAC}-{PapersOnLine}},
	shortjournal = {{IFAC}-{PapersOnLine}},
	author = {Wang, Dong and Pan, Quan and Zhao, Chunhui and Hu, Jinwen and Xu, Zhao and Yang, Feng and Zhou, Yihui},
	urldate = {2019-09-04},
	date = {2017-07-01},
	keywords = {Airborne Optical System, Array Arrangement, Camera Array, Imaging Properties, Synthetic Aperture Imaging},
	file = {ScienceDirect Full Text PDF:/home/ritzo/Zotero/storage/ZTQA5WU7/Wang et al. - 2017 - A Study on Camera Array and Its ApplicationsThis.pdf:application/pdf;ScienceDirect Snapshot:/home/ritzo/Zotero/storage/H7XJRMSG/S2405896317322681.html:text/html}
}

@article{blew_wirksamkeit_2018,
	title = {Wirksamkeit von Maßnahmen gegen Vogelkollisionen an Windenergieanlagen},
	volume = {{BfN}-Skripten},
	doi = {10.19217/skr518},
	pages = {129},
	author = {Blew, Jan and Grünkorn, Thomas and Reichenbach, Marc and Menke, Kerstin and Middeke, Oliver and Albrecht, Klaus and Bußler, Stefanie},
	date = {2018},
	langid = {german},
	file = {Blew et al. - Wirksamkeit von Maßnahmen gegen Vogelkollisionen a.pdf:/home/ritzo/Zotero/storage/8KXR4E7Z/Blew et al. - Wirksamkeit von Maßnahmen gegen Vogelkollisionen a.pdf:application/pdf}
}

@article{wei_lidar_2018,
	title = {{LiDAR} and Camera Detection Fusion in a Real-Time Industrial Multi-Sensor Collision Avoidance System},
	volume = {7},
	issn = {2079-9292},
	url = {http://www.mdpi.com/2079-9292/7/6/84},
	doi = {10.3390/electronics7060084},
	abstract = {Collision avoidance is a critical task in many applications, such as {ADAS} (advanced driver-assistance systems), industrial automation and robotics. In an industrial automation setting, certain areas should be off limits to an automated vehicle for protection of people and high-valued assets. These areas can be quarantined by mapping (e.g., {GPS}) or via beacons that delineate a no-entry area. We propose a delineation method where the industrial vehicle utilizes a {LiDAR} (Light Detection and Ranging) and a single color camera to detect passive beacons and model-predictive control to stop the vehicle from entering a restricted space. The beacons are standard orange trafﬁc cones with a highly reﬂective vertical pole attached. The {LiDAR} can readily detect these beacons, but suffers from false positives due to other reﬂective surfaces such as worker safety vests. Herein, we put forth a method for reducing false positive detection from the {LiDAR} by projecting the beacons in the camera imagery via a deep learning method and validating the detection using a neural network-learned projection from the camera to the {LiDAR} space. Experimental data collected at Mississippi State University’s Center for Advanced Vehicular Systems ({CAVS}) shows the effectiveness of the proposed system in keeping the true detection while mitigating false positives.},
	pages = {84},
	number = {6},
	journaltitle = {Electronics},
	shortjournal = {Electronics},
	author = {Wei, Pan and Cagle, Lucas and Reza, Tasmia and Ball, John and Gafford, James},
	urldate = {2019-09-04},
	date = {2018-05-30},
	langid = {english},
	file = {Wei et al. - 2018 - LiDAR and Camera Detection Fusion in a Real-Time I.pdf:/home/ritzo/Zotero/storage/AX7QAL7Y/Wei et al. - 2018 - LiDAR and Camera Detection Fusion in a Real-Time I.pdf:application/pdf}
}

@article{johnsen_real-time_nodate,
	title = {Real-Time Object Tracking and Classiﬁcation Using a Static Camera},
	abstract = {Understanding objects in video data is of particular interest due to its enhanced automation in public security surveillance as well as in trafﬁc control and pedestrian ﬂow analysis. Here, a system is presented which is able to detect and classify people and vehicles outdoors in different weather conditions using a static camera. The system is capable of correctly tracking multiple objects despite occlusions and object interactions. Results are presented on real world sequences and by online application of the algorithm.},
	pages = {6},
	author = {Johnsen, Swantje and Tews, Ashley},
	langid = {english},
	file = {Johnsen und Tews - Real-Time Object Tracking and Classiﬁcation Using .pdf:/home/ritzo/Zotero/storage/TDD7U96G/Johnsen und Tews - Real-Time Object Tracking and Classiﬁcation Using .pdf:application/pdf}
}

@inproceedings{takeki_detection_2016,
	location = {Phoenix, {AZ}, {USA}},
	title = {Detection of small birds in large images by combining a deep detector with semantic segmentation},
	isbn = {978-1-4673-9961-6},
	url = {http://ieeexplore.ieee.org/document/7533106/},
	doi = {10.1109/ICIP.2016.7533106},
	abstract = {This paper tackles the problem of bird detection in large landscape images for applications in the wind energy industry. While signiﬁcant progress in image recognition has been made by deep convolutional neural networks ({CNNs}), small object detection remains a problem. To solve it, we follow the idea that a detector can be tuned to small objects of interest and semantic segmentation methods can be complementarily used to recognize large background areas. Speciﬁcally, we train a {CNN}-based detector, fully convolutional networks, and a superpixel-based semantic segmentation method. The results of the three methods are combined by using support vector machines to achieve high detection performance. Experimental results on a bird image dataset show the high precision and effectiveness of the proposed method.},
	eventtitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {3977--3981},
	booktitle = {2016 {IEEE} International Conference on Image Processing ({ICIP})},
	publisher = {{IEEE}},
	author = {Takeki, Akito and Trinh, Tu Tuan and Yoshihashi, Ryota and Kawakami, Rei and Iida, Makoto and Naemura, Takeshi},
	urldate = {2019-09-04},
	date = {2016-09},
	langid = {english},
	file = {Takeki et al. - 2016 - Detection of small birds in large images by combin.pdf:/home/ritzo/Zotero/storage/G6X5VNB8/Takeki et al. - 2016 - Detection of small birds in large images by combin.pdf:application/pdf}
}

@book{goodfellow_deep_2016,
	location = {Cambridge, Massachusetts London, England},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	series = {Adaptive computation and machine learning},
	abstract = {Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models},
	pagetotal = {775},
	publisher = {The {MIT} Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	date = {2016},
	note = {{OCLC}: 955778308},
	file = {deeplearningbook.pdf:/home/ritzo/Zotero/storage/VHN2H99P/deeplearningbook.pdf:application/pdf;Table of Contents PDF:/home/ritzo/Zotero/storage/IMM4NLP9/Goodfellow et al. - 2016 - Deep learning.pdf:application/pdf}
}

@incollection{ferrari_sod-mtgan:_2018,
	location = {Cham},
	title = {{SOD}-{MTGAN}: Small Object Detection via Multi-Task Generative Adversarial Network},
	volume = {11217},
	isbn = {978-3-030-01260-1 978-3-030-01261-8},
	url = {http://link.springer.com/10.1007/978-3-030-01261-8_13},
	shorttitle = {{SOD}-{MTGAN}},
	abstract = {Object detection is a fundamental and important problem in computer vision. Although impressive results have been achieved on large/medium sized objects in large-scale detection benchmarks (e.g. the {COCO} dataset), the performance on small objects is far from satisfactory. The reason is that small objects lack suﬃcient detailed appearance information, which can distinguish them from the background or similar objects. To deal with the small object detection problem, we propose an end-to-end multi-task generative adversarial network ({MTGAN}). In the {MTGAN}, the generator is a super-resolution network, which can up-sample small blurred images into ﬁne-scale ones and recover detailed information for more accurate detection. The discriminator is a multitask network, which describes each super-resolved image patch with a real/fake score, object category scores, and bounding box regression oﬀsets. Furthermore, to make the generator recover more details for easier detection, the classiﬁcation and regression losses in the discriminator are back-propagated into the generator during training. Extensive experiments on the challenging {COCO} dataset demonstrate the eﬀectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods.},
	pages = {210--226},
	booktitle = {Computer Vision – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Bai, Yancheng and Zhang, Yongqiang and Ding, Mingli and Ghanem, Bernard},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	urldate = {2019-09-03},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-030-01261-8_13},
	file = {Bai et al. - 2018 - SOD-MTGAN Small Object Detection via Multi-Task G.pdf:/home/ritzo/Zotero/storage/S9Y72CCI/Bai et al. - 2018 - SOD-MTGAN Small Object Detection via Multi-Task G.pdf:application/pdf}
}

@article{szeliski_computer_nodate,
	title = {Computer Vision: Algorithms and Applications},
	pages = {979},
	author = {Szeliski, Richard},
	langid = {english}
}

@online{noauthor_accso_nodate,
	title = {Accso - Software Engineering und {IT}-Beratung},
	url = {https://accso.de/},
	abstract = {Accso ist der Spezialist für anspruchsvolle individuelle {IT}-Lösungen und hochkarätige Technologie- und Architekturberatung.},
	titleaddon = {Accso},
	urldate = {2019-10-10},
	langid = {german},
	file = {Snapshot:/home/ritzo/Zotero/storage/6HBGUJEB/accso.de.html:text/html}
}

@inproceedings{saez_cnn-based_2018,
	location = {Changshu},
	title = {{CNN}-based Fisheye Image Real-Time Semantic Segmentation},
	isbn = {978-1-5386-4452-2},
	url = {https://ieeexplore.ieee.org/document/8500456/},
	doi = {10.1109/IVS.2018.8500456},
	eventtitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1039--1044},
	booktitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	publisher = {{IEEE}},
	author = {Saez, Alvaro and Bergasa, Luis M. and Romeral, Eduardo and Lopez, Elena and Barea, Rafael and Sanz, Rafael},
	urldate = {2019-10-10},
	date = {2018-06},
	langid = {english},
	file = {Saez et al. - 2018 - CNN-based Fisheye Image Real-Time Semantic Segment.pdf:/home/ritzo/Zotero/storage/K89TETK2/Saez et al. - 2018 - CNN-based Fisheye Image Real-Time Semantic Segment.pdf:application/pdf}
}

@article{zhu_object_2019,
	title = {Object detection and localization in 3D environment by fusing raw fisheye image and attitude data},
	volume = {59},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320319300069},
	doi = {10.1016/j.jvcir.2019.01.005},
	abstract = {In robotic systems, the ﬁsheye camera can provide a large ﬁeld of view ({FOV}). Usually, the traditional restoring algorithms are needed, which are computational heavy and will introduce noise into original data, since the ﬁsheye images are distorted. In this paper, we propose a framework to detect objects from the raw ﬁsheye images without restoration, then locate objects in the real world coordinate by fusing attitude information. A deep neural network architecture based on the {MobileNet} and feature pyramid structure is designed to detect targets directly on the ﬁsheye raw images. Then, the target can be located based on the ﬁsheye visual model and the attitude of the camera. Compared to traditional approaches, this approach has advantages in computational efﬁciency and accuracy. This approach is validated by experiments with a ﬁsheye camera and an onboard computer on a micro-aerial vehicle ({MAV}).},
	pages = {128--139},
	journaltitle = {Journal of Visual Communication and Image Representation},
	shortjournal = {Journal of Visual Communication and Image Representation},
	author = {Zhu, Jun and Zhu, Jiangcheng and Wan, Xudong and Wu, Chao and Xu, Chao},
	urldate = {2019-10-12},
	date = {2019-02},
	langid = {english},
	file = {zhu2019.pdf:/home/ritzo/Downloads/zhu2019.pdf:application/pdf}
}

@article{hu_finding_2016,
	title = {Finding Tiny Faces},
	url = {http://arxiv.org/abs/1612.04402},
	abstract = {Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99\% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets ({FDDB} and {WIDER} {FACE}). In particular, when compared to prior art on {WIDER} {FACE}, our results reduce error by a factor of 2 (our models produce an {AP} of 82\% while prior art ranges from 29-64\%).},
	journaltitle = {{arXiv}:1612.04402 [cs]},
	author = {Hu, Peiyun and Ramanan, Deva},
	urldate = {2019-10-14},
	date = {2016-12-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.04402},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Hu and Ramanan - 2016 - Finding Tiny Faces.pdf:/home/ritzo/Zotero/storage/8DIULQYP/Hu and Ramanan - 2016 - Finding Tiny Faces.pdf:application/pdf}
}

@article{fu_dssd_2017,
	title = {{DSSD} : Deconvolutional Single Shot Detector},
	url = {http://arxiv.org/abs/1701.06659},
	shorttitle = {{DSSD}},
	abstract = {The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we ﬁrst combine a state-ofthe-art classiﬁer (Residual-101 [14]) with a fast detection framework ({SSD} [18]). We then augment {SSD}+Residual101 with deconvolution layers to introduce additional largescale context in object detection and improve accuracy, especially for small objects, calling our resulting system {DSSD} for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, speciﬁcally a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both {PASCAL} {VOC} and {COCO} detection. Our {DSSD} with 513 × 513 input achieves 81.5\% {mAP} on {VOC}2007 test, 80.0\% {mAP} on {VOC}2012 test, and 33.2\% {mAP} on {COCO}, outperforming a state-of-the-art method R-{FCN} [3] on each dataset.},
	journaltitle = {{arXiv}:1701.06659 [cs]},
	author = {Fu, Cheng-Yang and Liu, Wei and Ranga, Ananth and Tyagi, Ambrish and Berg, Alexander C.},
	urldate = {2019-10-14},
	date = {2017-01-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.06659},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Fu et al. - 2017 - DSSD  Deconvolutional Single Shot Detector.pdf:/home/ritzo/Zotero/storage/KTXIPK9I/Fu et al. - 2017 - DSSD  Deconvolutional Single Shot Detector.pdf:application/pdf}
}

@online{ouaknine_review_2018,
	title = {Review of Deep Learning Algorithms for Object Detection},
	url = {https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852},
	abstract = {Why object detection instead of image classification?},
	titleaddon = {Medium},
	author = {Ouaknine, Arthur},
	urldate = {2019-10-15},
	date = {2018-02-05},
	langid = {english},
	file = {Snapshot:/home/ritzo/Zotero/storage/MZCQXFIB/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	pages = {1097--1105},
	booktitle = {Advances in Neural Information Processing Systems 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	urldate = {2019-10-15},
	date = {2012},
	file = {NIPS Full Text PDF:/home/ritzo/Zotero/storage/98JIDMRG/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:/home/ritzo/Zotero/storage/94QRLXJT/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{kirillov_unified_nodate,
	title = {A Unified Architecture for Instance and Semantic Segmentation},
	pages = {48},
	author = {Kirillov, Alexander and Girshick, Ross and Dollár, Piotr},
	langid = {english},
	file = {Kirillov et al. - A Unified Architecture for Instance and Semantic S.pdf:/home/ritzo/Zotero/storage/NG5HC8JL/Kirillov et al. - A Unified Architecture for Instance and Semantic S.pdf:application/pdf}
}

@article{tabernik_deep_2019,
	title = {Deep Learning for Large-Scale Traffic-Sign Detection and Recognition},
	url = {http://arxiv.org/abs/1904.00649},
	abstract = {Automatic detection and recognition of trafﬁc signs plays a crucial role in management of the trafﬁc-sign inventory. It provides accurate and timely way to manage trafﬁc-sign inventory with a minimal human effort. In the computer vision community the recognition and detection of trafﬁc signs is a well-researched problem. A vast majority of existing approaches perform well on trafﬁc signs needed for advanced driversassistance and autonomous systems. However, this represents a relatively small number of all trafﬁc signs (around 50 categories out of several hundred) and performance on the remaining set of trafﬁc signs, which are required to eliminate the manual labor in trafﬁc-sign inventory management, remains an open question. In this paper, we address the issue of detecting and recognizing a large number of trafﬁc-sign categories suitable for automating trafﬁc-sign inventory management. We adopt a convolutional neural network ({CNN}) approach, the Mask R-{CNN}, to address the full pipeline of detection and recognition with automatic end-to-end learning. We propose several improvements that are evaluated on the detection of trafﬁc signs and result in an improved overall performance. This approach is applied to detection of 200 trafﬁc-sign categories represented in our novel dataset. Results are reported on highly challenging trafﬁcsign categories that have not yet been considered in previous works. We provide comprehensive analysis of the deep learning method for the detection of trafﬁc signs with large intra-category appearance variation and show below 3\% error rates with the proposed approach, which is sufﬁcient for deployment in practical applications of trafﬁc-sign inventory management.},
	journaltitle = {{arXiv}:1904.00649 [cs]},
	author = {Tabernik, Domen and Skočaj, Danijel},
	urldate = {2019-10-21},
	date = {2019-04-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.00649},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1904.00649.pdf:/home/ritzo/Zotero/storage/JJBL8CQ2/1904.00649.pdf:application/pdf}
}

@collection{sammut_encyclopedia_2017,
	location = {Boston, {MA}},
	title = {Encyclopedia of Machine Learning and Data Mining},
	isbn = {978-1-4899-7685-7 978-1-4899-7687-1},
	url = {http://link.springer.com/10.1007/978-1-4899-7687-1},
	publisher = {Springer {US}},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	urldate = {2019-10-21},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-1-4899-7687-1},
	file = {Sammut and Webb - 2017 - Encyclopedia of Machine Learning and Data Mining.pdf:/home/ritzo/Zotero/storage/92CLA369/Sammut and Webb - 2017 - Encyclopedia of Machine Learning and Data Mining.pdf:application/pdf}
}

@inproceedings{lin_feature_2017,
	location = {Honolulu, {HI}},
	title = {Feature Pyramid Networks for Object Detection},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network ({FPN}), shows signiﬁcant improvement as a generic feature extractor in several applications. Using {FPN} in a basic Faster R-{CNN} system, our method achieves state-of-the-art singlemodel results on the {COCO} detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the {COCO} 2016 challenge winners. In addition, our method can run at 5 {FPS} on a {GPU} and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {936--944},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	urldate = {2019-10-16},
	date = {2017-07},
	langid = {english},
	file = {Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:/home/ritzo/Zotero/storage/WNYMV26M/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf}
}

@article{kern_automatic_nodate,
	title = {Automatic Sleep Stage Classiﬁcation using Convolutional Neural Networks with Long Short-Term Memory},
	pages = {48},
	author = {Kern, Simon Johannes},
	langid = {english},
	file = {Kern - Automatic Sleep Stage Classiﬁcation using Convolut.pdf:/home/ritzo/Zotero/storage/BPJSJI65/Kern - Automatic Sleep Stage Classiﬁcation using Convolut.pdf:application/pdf}
}

@article{shafiee_fast_2017,
	title = {Fast {YOLO}: A Fast You Only Look Once System for Real-time Embedded Object Detection in Video},
	url = {http://arxiv.org/abs/1709.05943},
	shorttitle = {Fast {YOLO}},
	abstract = {Object detection is considered one of the most challenging problems in this field of computer vision, as it involves the combination of object classification and object localization within a scene. Recently, deep neural networks ({DNNs}) have been demonstrated to achieve superior object detection performance compared to other approaches, with {YOLOv}2 (an improved You Only Look Once model) being one of the state-of-the-art in {DNN}-based object detection methods in terms of both speed and accuracy. Although {YOLOv}2 can achieve real-time performance on a powerful {GPU}, it still remains very challenging for leveraging this approach for real-time object detection in video on embedded computing devices with limited computational power and limited memory. In this paper, we propose a new framework called Fast {YOLO}, a fast You Only Look Once framework which accelerates {YOLOv}2 to be able to perform object detection in video on embedded devices in a real-time manner. First, we leverage the evolutionary deep intelligence framework to evolve the {YOLOv}2 network architecture and produce an optimized architecture (referred to as O-{YOLOv}2 here) that has 2.8X fewer parameters with just a {\textasciitilde}2\% {IOU} drop. To further reduce power consumption on embedded devices while maintaining performance, a motion-adaptive inference method is introduced into the proposed Fast {YOLO} framework to reduce the frequency of deep inference with O-{YOLOv}2 based on temporal motion characteristics. Experimental results show that the proposed Fast {YOLO} framework can reduce the number of deep inferences by an average of 38.13\%, and an average speedup of {\textasciitilde}3.3X for objection detection in video compared to the original {YOLOv}2, leading Fast {YOLO} to run an average of {\textasciitilde}18FPS on a Nvidia Jetson {TX}1 embedded system.},
	journaltitle = {{arXiv}:1709.05943 [cs]},
	author = {Shafiee, Mohammad Javad and Chywl, Brendan and Li, Francis and Wong, Alexander},
	urldate = {2019-10-29},
	date = {2017-09-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.05943},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {Shafiee et al. - 2017 - Fast YOLO A Fast You Only Look Once System for Re.pdf:/home/ritzo/Zotero/storage/3AZRB35A/Shafiee et al. - 2017 - Fast YOLO A Fast You Only Look Once System for Re.pdf:application/pdf}
}

@inproceedings{tijtgat_embedded_2017,
	location = {Venice, Italy},
	title = {Embedded Real-Time Object Detection for a {UAV} Warning System},
	isbn = {978-1-5386-1034-3},
	url = {http://ieeexplore.ieee.org/document/8265457/},
	doi = {10.1109/ICCVW.2017.247},
	abstract = {In this paper, we demonstrate and evaluate a method to perform real-time object detection on-board a {UAV} using the state of the art {YOLOv}2 object detection algorithm running on an {NVIDIA} Jetson {TX}2, an {GPU} platform targeted at power constrained mobile applications that use neural networks under the hood. This, as a result of comparing several cutting edge object detection algorithms. Multiple evaluations we present provide insights that help choose the optimal object detection conﬁguration given certain frame rate and detection accuracy requirements. We propose how this setup running on-board a {UAV} can be used to process a video feed during emergencies in real-time, and feed a decision support warning system using the generated detections.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision Workshop ({ICCVW})},
	pages = {2110--2118},
	booktitle = {2017 {IEEE} International Conference on Computer Vision Workshops ({ICCVW})},
	publisher = {{IEEE}},
	author = {Tijtgat, Nils and Ranst, Wiebe Van and Volckaert, Bruno and Goedeme, Toon and Turck, Filip De},
	urldate = {2019-10-29},
	date = {2017-10},
	langid = {english},
	file = {Tijtgat et al. - 2017 - Embedded Real-Time Object Detection for a UAV Warn.pdf:/home/ritzo/Zotero/storage/I5KB5J43/Tijtgat et al. - 2017 - Embedded Real-Time Object Detection for a UAV Warn.pdf:application/pdf}
}

@article{hossain_deep_2019,
	title = {Deep Learning-Based Real-Time Multiple-Object Detection and Tracking from Aerial Imagery via a Flying Robot with {GPU}-Based Embedded Devices},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/15/3371},
	doi = {10.3390/s19153371},
	abstract = {In recent years, demand has been increasing for target detection and tracking from aerial imagery via drones using onboard powered sensors and devices. We propose a very effective method for this application based on a deep learning framework. A state-of-the-art embedded hardware system empowers small flying robots to carry out the real-time onboard computation necessary for object tracking. Two types of embedded modules were developed: one was designed using a Jetson {TX} or {AGX} Xavier, and the other was based on an Intel Neural Compute Stick. These are suitable for real-time onboard computing power on small flying drones with limited space. A comparative analysis of current state-of-the-art deep learning-based multi-object detection algorithms was carried out utilizing the designated {GPU}-based embedded computing modules to obtain detailed metric data about frame rates, as well as the computation power. We also introduce an effective target tracking approach for moving objects. The algorithm for tracking moving objects is based on the extension of simple online and real-time tracking. It was developed by integrating a deep learning-based association metric approach with simple online and real-time tracking (Deep {SORT}), which uses a hypothesis tracking methodology with Kalman filtering and a deep learningbased association metric. In addition, a guidance system that tracks the target position using a {GPUbased} algorithm is introduced. Finally, we demonstrate the effectiveness of the proposed algorithms by real-time experiments with a small multi-rotor drone.},
	pages = {3371},
	number = {15},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {{Hossain} and {Lee}},
	urldate = {2019-10-29},
	date = {2019-07-31},
	langid = {english},
	file = {Hossain and Lee - 2019 - Deep Learning-Based Real-Time Multiple-Object Dete.pdf:/home/ritzo/Zotero/storage/EJPXMMFZ/Hossain and Lee - 2019 - Deep Learning-Based Real-Time Multiple-Object Dete.pdf:application/pdf}
}

@inproceedings{ruzicka_fast_2018,
	location = {Waltham, {MA}},
	title = {Fast and accurate object detection in high resolution 4K and 8K video using {GPUs}},
	isbn = {978-1-5386-5989-2},
	url = {https://ieeexplore.ieee.org/document/8547574/},
	doi = {10.1109/HPEC.2018.8547574},
	abstract = {Machine learning has celebrated a lot of achievements on computer vision tasks such as object detection, but the traditionally used models work with relatively low resolution images. The resolution of recording devices is gradually increasing and there is a rising need for new methods of processing high resolution data. We propose an attention pipeline method which uses two staged evaluation of each image or video frame under rough and reﬁned resolution to limit the total number of necessary evaluations. For both stages, we make use of the fast object detection model {YOLO} v2. We have implemented our model in code, which distributes the work across {GPUs}. We maintain high accuracy while reaching the average performance of 3-6 fps on 4K video and 2 fps on 8K video.},
	eventtitle = {2018 {IEEE} High Performance Extreme Computing Conference ({HPEC})},
	pages = {1--7},
	booktitle = {2018 {IEEE} High Performance extreme Computing Conference ({HPEC})},
	publisher = {{IEEE}},
	author = {Ruzicka, Vit and Franchetti, Franz},
	urldate = {2019-10-29},
	date = {2018-09},
	langid = {english},
	file = {Ruzicka and Franchetti - 2018 - Fast and accurate object detection in high resolut.pdf:/home/ritzo/Zotero/storage/BBCJ8S2D/Ruzicka and Franchetti - 2018 - Fast and accurate object detection in high resolut.pdf:application/pdf}
}

@inproceedings{lee_rdfnet:_2017,
	location = {Venice},
	title = {{RDFNet}: {RGB}-D Multi-level Residual Feature Fusion for Indoor Semantic Segmentation},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237795/},
	doi = {10.1109/ICCV.2017.533},
	shorttitle = {{RDFNet}},
	abstract = {In multi-class indoor semantic segmentation using {RGBD} data, it has been shown that incorporating depth feature into {RGB} feature is helpful to improve segmentation accuracy. However, previous studies have not fully exploited the potentials of multi-modal feature fusion, e.g., simply concatenating {RGB} and depth features or averaging {RGB} and depth score maps. To learn the optimal fusion of multimodal features, this paper presents a novel network that extends the core idea of residual learning to {RGB}-D semantic segmentation. Our network effectively captures multilevel {RGB}-D {CNN} features by including multi-modal feature fusion blocks and multi-level feature reﬁnement blocks. Feature fusion blocks learn residual {RGB} and depth features and their combinations to fully exploit the complementary characteristics of {RGB} and depth data. Feature reﬁnement blocks learn the combination of fused features from multiple levels to enable high-resolution prediction. Our network can efﬁciently train discriminative multi-level features from each modality end-to-end by taking full advantage of skip-connections. Our comprehensive experiments demonstrate that the proposed architecture achieves the state-of-the-art accuracy on two challenging {RGB}-D indoor datasets, {NYUDv}2 and {SUN} {RGB}-D.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {4990--4999},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Lee, Seungyong and Park, Seong-Jin and Hong, Ki-Sang},
	urldate = {2019-10-28},
	date = {2017-10},
	langid = {english},
	file = {Lee et al. - 2017 - RDFNet RGB-D Multi-level Residual Feature Fusion .pdf:/home/ritzo/Zotero/storage/EZS6L6LI/Lee et al. - 2017 - RDFNet RGB-D Multi-level Residual Feature Fusion .pdf:application/pdf}
}

@article{rojczyk_automatisierte_nodate,
	title = {Automatisierte Konstruktion von künstlichen neuronalen Netzen mithilfe von bestärkendem Lernen},
	pages = {95},
	author = {Rojczyk, Kevin and Darmstadt, Hochschule and Mathematik, Fachbereiche},
	langid = {german},
	file = {Rojczyk et al. - Automatisierte Konstruktion von künstlichen neuron.pdf:/home/ritzo/Zotero/storage/84P4MWUB/Rojczyk et al. - Automatisierte Konstruktion von künstlichen neuron.pdf:application/pdf}
}

@article{zhao_object_2019,
	title = {Object Detection with Deep Learning: A Review},
	url = {http://arxiv.org/abs/1807.05511},
	shorttitle = {Object Detection with Deep Learning},
	abstract = {Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network ({CNN}). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.},
	journaltitle = {{arXiv}:1807.05511 [cs]},
	author = {Zhao, Zhong-Qiu and Zheng, Peng and Xu, Shou-tao and Wu, Xindong},
	urldate = {2019-10-30},
	date = {2019-04-16},
	eprinttype = {arxiv},
	eprint = {1807.05511},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/VF5SX5NW/Zhao et al. - 2019 - Object Detection with Deep Learning A Review.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/G5GYPM4I/1807.html:text/html}
}

@article{liu_multispectral_2016,
	title = {Multispectral Deep Neural Networks for Pedestrian Detection},
	url = {http://arxiv.org/abs/1611.02644},
	abstract = {Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-{CNN} for multispectral pedestrian detection task and then model it into a convolutional network ({ConvNet}) fusion problem. Further, we discover that {ConvNet}-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in {DNNs} simultaneously. We carefully design four {ConvNet} fusion architectures that integrate two-branch {ConvNets} on different {DNNs} stages, all of which yield better performance compared with the baseline detector. Our experimental results on {KAIST} pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 11\% and yields a missing rate 3.5\% lower than the other proposed architectures.},
	journaltitle = {{arXiv}:1611.02644 [cs]},
	author = {Liu, Jingjing and Zhang, Shaoting and Wang, Shu and Metaxas, Dimitris N.},
	urldate = {2019-10-30},
	date = {2016-11-08},
	eprinttype = {arxiv},
	eprint = {1611.02644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/34B689XR/Liu et al. - 2016 - Multispectral Deep Neural Networks for Pedestrian .pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/CLYHH6AT/1611.html:text/html}
}

@inproceedings{mhalla_multi-object_2018,
	title = {Multi-object detection and tracking in video sequences. (Détection et suivi multi-objets dans des séquences vidéo)},
	abstract = {The work developed in this {PhD} thesis is focused on video sequence analysis. Thelatter consists of object detection, categorization and tracking. The development ofreliable solutions for the analysis of video sequences opens new horizons for severalapplications such as intelligent transport systems, video surveillance and robotics.In this thesis, we put forward several contributions to deal with the problems ofdetecting and tracking multi-objects on video sequences. The proposed frameworksare based on deep learning networks and transfer learning approaches.In a first contribution, we tackle the problem of multi-object detection by puttingforward a new transfer learning framework based on the formalism and the theoryof a Sequential Monte Carlo ({SMC}) filter to automatically specialize a Deep {ConvolutionalNeural} Network ({DCNN}) detector towards a target scene. The suggestedspecialization framework is used in order to transfer the knowledge from the sourceand the target domain to the target scene and to estimate the unknown target distributionas a specialized dataset composed of samples from the target domain. Thesesamples are selected according to the importance of their weights which reflectsthe likelihood that they belong to the target distribution. The obtained specializeddataset allows training a specialized {DCNN} detector to a target scene withouthuman intervention.In a second contribution, we propose an original multi-object tracking frameworkbased on spatio-temporal strategies (interlacing/inverse interlacing) and aninterlaced deep detector, which improves the performances of tracking-by-detectionalgorithms and helps to track objects in complex videos (occlusion, intersection,strong motion).In a third contribution, we provide an embedded system for traffic surveillance,which integrates an extension of the {SMC} framework so as to improve the detectionaccuracy in both day and night conditions and to specialize any {DCNN} detector forboth mobile and stationary cameras.Throughout this report, we provide both quantitative and qualitative results.On several aspects related to video sequence analysis, this work outperformsthe state-of-the-art detection and tracking frameworks. In addition, we havesuccessfully implemented our frameworks in an embedded hardware platform forroad traffic safety and monitoring.},
	author = {Mhalla, Ala},
	date = {2018},
	keywords = {Deep learning, Categorization, Closed-circuit television, Day and Night (cellular automaton), Embedded system, Interlacing (bitmaps), Monte Carlo method, Object detection, Performance, Robotics, Semantics (computer science), Sequence analysis, Stationary process},
	file = {Full Text PDF:/home/ritzo/Zotero/storage/S5V2X3PN/Mhalla - 2018 - Multi-object detection and tracking in video seque.pdf:application/pdf}
}

@article{mao_towards_2018,
	title = {Towards Real-Time Object Detection on Embedded Systems},
	volume = {6},
	issn = {2168-6750, 2376-4562},
	doi = {10.1109/TETC.2016.2593643},
	abstract = {Convolutional neural network ({CNN}) based methods have achieved great success in image classification and object detection tasks. However, unlike the image classification task, object detection is much more computation-intensive and energy-consuming since a large number of possible object proposals need to be evaluated. Consequently, it is difficult for object detection methods to be integrated into embedded systems with limited computing resources and energy supply. In this paper, we propose a pipelined object detection implementation on the embedded platform. We present a comprehensive analysis of state-of-the-art object detection algorithms and select Fast R-{CNN} as a possible solution. Additional modifications on the Fast R-{CNN} method are made to fit the specific platform and achieve trade-off between speed and accuracy on embedded systems. Finally, a multi-stage pipelined implementation on the embedded {CPU}+{GPU} platform with duplicated module-parallelism is proposed to make full use of the limited computation resources. The proposed system is highly energy-efficient and close to real-time performance. In the first Low-Power Image Recognition Challenge ({LPIRC}), our system achieved the best result with {mAP}/Energy of 1.818e-2/ (W.h) on the embedded Jetson {TK}1 {CPU}+{GPU} platform.},
	pages = {417--431},
	number = {3},
	journaltitle = {{IEEE} Transactions on Emerging Topics in Computing},
	author = {Mao, Huizi and Yao, Song and Tang, Tianqi and Li, Boxun and Yao, Jun and Wang, Yu},
	date = {2018-07},
	keywords = {Object detection, Algorithm design and analysis, Benchmark testing, computation resources, computing resources, convolutional neural network ({CNN}), convolutional neural network based methods, embedded {CPU}+{GPU} platform, embedded Jetson {TK}1 {CPU}+{GPU} platform, embedded platform, embedded system, embedded systems, Embedded systems, energy supply, energy-efficient, Fast R-{CNN} method, graphics processing units, Hardware, image classification, image classification task, Image recognition, low-power, Low-Power Image Recognition Challenge, {mAP}/Energy, multistage pipelined implementation, neural nets, object detection, object detection tasks, object proposals, object recognition, pipeline, pipelined object detection implementation, Prediction algorithms, real-time object detection, specific platform},
	file = {IEEE Xplore Abstract Record:/home/ritzo/Zotero/storage/KMXKXMZ9/7543495.html:text/html;IEEE Xplore Full Text PDF:/home/ritzo/Zotero/storage/AA6IX2D3/Mao et al. - 2018 - Towards Real-Time Object Detection on Embedded Sys.pdf:application/pdf}
}

@article{tome_deep_2016,
	title = {Deep convolutional neural networks for pedestrian detection},
	volume = {47},
	issn = {09235965},
	url = {http://arxiv.org/abs/1510.03608},
	doi = {10.1016/j.image.2016.05.007},
	abstract = {Pedestrian detection is a popular research topic due to its paramount importance for a number of applications, especially in the fields of automotive, surveillance and robotics. Despite the significant improvements, pedestrian detection is still an open challenge that calls for more and more accurate algorithms. In the last few years, deep learning and in particular convolutional neural networks emerged as the state of the art in terms of accuracy for a number of computer vision tasks such as image classification, object detection and segmentation, often outperforming the previous gold standards by a large margin. In this paper, we propose a pedestrian detection system based on deep learning, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline we propose an architecture that outperforms traditional methods, achieving a task accuracy close to that of state-of-the-art approaches, while requiring a low computational time. Finally, we tested the system on an {NVIDIA} Jetson {TK}1, a 192-core platform that is envisioned to be a forerunner computational brain of future self-driving cars.},
	pages = {482--489},
	journaltitle = {Signal Processing: Image Communication},
	shortjournal = {Signal Processing: Image Communication},
	author = {Tomè, Denis and Monti, Federico and Baroffio, Luca and Bondi, Luca and Tagliasacchi, Marco and Tubaro, Stefano},
	urldate = {2019-10-30},
	date = {2016-09},
	eprinttype = {arxiv},
	eprint = {1510.03608},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/PKXMIGN8/Tomè et al. - 2016 - Deep convolutional neural networks for pedestrian .pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/U8VV2F34/1510.html:text/html}
}

@inproceedings{lee_wide-residual-inception_2017,
	title = {Wide-residual-inception networks for real-time object detection},
	doi = {10.1109/IVS.2017.7995808},
	abstract = {Since convolutional neural network ({CNN}) models emerged, several tasks in computer vision have actively deployed {CNN} models for feature extraction. However, the conventional {CNN} models have a high computational cost and require high memory capacity, which is impractical and unaffordable for commercial applications such as real-time on-road object detection on embedded boards or mobile platforms. To tackle this limitation of {CNN} models, this paper proposes a wide-residual-inception ({WR}-Inception) network, which constructs the architecture based on a residual inception unit that captures objects of various sizes on the same feature map, as well as shallower and wider layers, compared to state-of-the-art networks like {ResNets}. To verify the proposed networks, this paper conducted two experiments; one is a classification task on {CIFAR}-10/100 and the other is an on-road object detection task using a Single-Shot Multi-box Detector ({SSD}) on the {KITTI} dataset. {WR}-Inception achieves comparable accuracy on {CIFAR}-10/100, with test errors at 4.82\% and 23.12\%, respectively, which outperforms 164-layer Pre-{ResNets}. In addition, the detection experiments demonstrate that the {WR}-Inception-based {SSD} outperforms {ResNet}-101 - based {SSD} on {KITTI}. Besides, {WR}-Inception-based {SSD} achieves 16 frames per seconds, which is 3.85 times faster than {ResNet}-101-based {SSD}. We could expect {WR}-Inception to be used for real application systems.},
	eventtitle = {2017 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {758--764},
	booktitle = {2017 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Lee, Youngwan and Kim, Huieun and Park, Eunsoo and Cui, Xuenan and Kim, Hakil},
	date = {2017-06},
	keywords = {Object detection, object detection, real-time object detection, {CIFAR}-10-100, {CNN}, Computational modeling, computer vision, convolutional neural network models, Detectors, embedded boards, feature extraction, Feature extraction, feedforward neural nets, {KITTI} dataset, memory capacity, Memory management, mobile platforms, Neural networks, on-road object detection, on-road object detection task, Real-time systems, {ResNets}, single-shot multibox detector, traffic engineering computing, wide-residual-inception networks, {WR}-Inception, {WR}-Inception-based {SSD}},
	file = {IEEE Xplore Abstract Record:/home/ritzo/Zotero/storage/W5L5N35R/7995808.html:text/html;IEEE Xplore Full Text PDF:/home/ritzo/Zotero/storage/6G2MBSWV/Lee et al. - 2017 - Wide-residual-inception networks for real-time obj.pdf:application/pdf}
}

@article{zhang_how_2016,
	title = {How Far are We from Solving Pedestrian Detection?},
	url = {http://arxiv.org/abs/1602.01237},
	abstract = {Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the “perfect single frame detector”. We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterise both localisation and background-versusforeground errors.},
	journaltitle = {{arXiv}:1602.01237 [cs]},
	author = {Zhang, Shanshan and Benenson, Rodrigo and Omran, Mohamed and Hosang, Jan and Schiele, Bernt},
	urldate = {2019-10-31},
	date = {2016-06-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1602.01237},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhang et al. - 2016 - How Far are We from Solving Pedestrian Detection.pdf:/home/ritzo/Zotero/storage/UY4S5FGP/Zhang et al. - 2016 - How Far are We from Solving Pedestrian Detection.pdf:application/pdf}
}

@incollection{agapito_ten_2015,
	location = {Cham},
	title = {Ten Years of Pedestrian Detection, What Have We Learned?},
	volume = {8926},
	isbn = {978-3-319-16180-8 978-3-319-16181-5},
	url = {http://link.springer.com/10.1007/978-3-319-16181-5_47},
	abstract = {Paper-by-paper results make it easy to miss the forest for the trees.We analyse the remarkable progress of the last decade by discussing the main ideas explored in the 40+ detectors currently present in the Caltech pedestrian detection benchmark. We observe that there exist three families of approaches, all currently reaching similar detection quality. Based on our analysis, we study the complementarity of the most promising ideas by combining multiple published strategies. This new decision forest detector achieves the current best known performance on the challenging Caltech-{USA} dataset.},
	pages = {613--627},
	booktitle = {Computer Vision - {ECCV} 2014 Workshops},
	publisher = {Springer International Publishing},
	author = {Benenson, Rodrigo and Omran, Mohamed and Hosang, Jan and Schiele, Bernt},
	editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
	urldate = {2019-10-31},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-16181-5_47},
	file = {Benenson et al. - 2015 - Ten Years of Pedestrian Detection, What Have We Le.pdf:/home/ritzo/Zotero/storage/597H59DQ/Benenson et al. - 2015 - Ten Years of Pedestrian Detection, What Have We Le.pdf:application/pdf}
}

@inproceedings{benenson_pedestrian_2012,
	location = {Providence, {RI}},
	title = {Pedestrian detection at 100 frames per second},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6248017/},
	doi = {10.1109/CVPR.2012.6248017},
	abstract = {We present a new pedestrian detector that improves both in speed and quality over state-of-the-art. By efﬁciently handling different scales and transferring computation from test time to training time, detection speed is improved. When processing monocular images, our system provides high quality detections at 50 fps.},
	eventtitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {2903--2910},
	booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	publisher = {{IEEE}},
	author = {Benenson, R. and Mathias, M. and Timofte, R. and Van Gool, L.},
	urldate = {2019-10-31},
	date = {2012-06},
	langid = {english},
	file = {Benenson et al. - 2012 - Pedestrian detection at 100 frames per second.pdf:/home/ritzo/Zotero/storage/HT6ESNT3/Benenson et al. - 2012 - Pedestrian detection at 100 frames per second.pdf:application/pdf}
}

@article{zhang_is_2016,
	title = {Is Faster R-{CNN} Doing Well for Pedestrian Detection?},
	url = {http://arxiv.org/abs/1607.07032},
	abstract = {Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-{CNN} [1,2] have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-{CNN} [2] for pedestrian detection. We discover that the Region Proposal Network ({RPN}) in Faster R-{CNN} indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classiﬁer degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insuﬃcient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but eﬀective baseline for pedestrian detection, using an {RPN} followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, {INRIA}, {ETH}, and {KITTI}), presenting competitive accuracy and good speed. Code will be made publicly available.},
	journaltitle = {{arXiv}:1607.07032 [cs]},
	author = {Zhang, Liliang and Lin, Liang and Liang, Xiaodan and He, Kaiming},
	urldate = {2019-10-31},
	date = {2016-07-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1607.07032},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 68U01},
	file = {Zhang et al. - 2016 - Is Faster R-CNN Doing Well for Pedestrian Detectio.pdf:/home/ritzo/Zotero/storage/4QMETGYQ/Zhang et al. - 2016 - Is Faster R-CNN Doing Well for Pedestrian Detectio.pdf:application/pdf}
}

@article{hanhirova_latency_2018,
	title = {Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision},
	url = {http://arxiv.org/abs/1803.09492},
	abstract = {We study performance characteristics of convolutional neural networks ({CNN}) for mobile computer vision systems. {CNNs} have proven to be a powerful and efficient approach to implement such systems. However, the system performance depends largely on the utilization of hardware accelerators, which are able to speed up the execution of the underlying mathematical operations tremendously through massive parallelism. Our contribution is performance characterization of multiple {CNN}-based models for object recognition and detection with several different hardware platforms and software frameworks, using both local (on-device) and remote (network-side server) computation. The measurements are conducted using real workloads and real processing platforms. On the platform side, we concentrate especially on {TensorFlow} and {TensorRT}. Our measurements include embedded processors found on mobile devices and high-performance processors that can be used on the network side of mobile systems. We show that there exists significant latency--throughput trade-offs but the behavior is very complex. We demonstrate and discuss several factors that affect the performance and yield this complex behavior.},
	journaltitle = {{arXiv}:1803.09492 [cs]},
	author = {Hanhirova, Jussi and Kämäräinen, Teemu and Seppälä, Sipi and Siekkinen, Matti and Hirvisalo, Vesa and Ylä-Jääski, Antti},
	urldate = {2019-11-04},
	date = {2018-03-26},
	eprinttype = {arxiv},
	eprint = {1803.09492},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/HMGU2F7M/Hanhirova et al. - 2018 - Latency and Throughput Characterization of Convolu.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/AXTQ9SFH/1803.html:text/html}
}

@article{aslani_optical_2013,
	title = {Optical Flow Based Moving Object Detection and Tracking for Traffic Surveillance},
	volume = {7},
	abstract = {Automated motion detection and tracking is a challenging task in traffic surveillance. In this paper, a system is developed to gather useful information from stationary cameras for detecting moving objects in digital videos. The moving detection and tracking system is developed based on optical flow estimation together with application and combination of various relevant computer vision and image processing techniques to enhance the process. To remove noises, median filter is used and the unwanted objects are removed by applying thresholding algorithms in morphological operations. Also the object type restrictions are set using blob analysis. The results show that the proposed system successfully detects and tracks moving objects in urban videos.},
	pages = {5},
	number = {9},
	author = {Aslani, Sepehr and Mahdavi-Nasab, Homayoun},
	date = {2013},
	langid = {english},
	file = {Aslani and Mahdavi-Nasab - 2013 - Optical Flow Based Moving Object Detection and Tra.pdf:/home/ritzo/Zotero/storage/F793RXQ8/Aslani and Mahdavi-Nasab - 2013 - Optical Flow Based Moving Object Detection and Tra.pdf:application/pdf}
}

@article{nousi_embedded_nodate,
	title = {{EMBEDDED} {UAV} {REAL}-{TIME} {VISUAL} {OBJECT} {DETECTION} {AND} {TRACKING}},
	abstract = {The use of camera-equipped Unmanned Aerial Vehicles ({UAVs}, or “drones”) for a wide range of aerial video capturing applications, including media production, surveillance, search and rescue operations, etc., has exploded in recent years. Technological progress has led to commercially available {UAVs} with a degree of cognitive autonomy and perceptual capabilities, such as automated, on-line detection and tracking of target objects upon the captured footage. However, the limited computational hardware, the possibly high camera-to-target distance and the fact that both the {UAV}/camera and the target(s) are moving, makes it challenging to achieve both high accuracy and stable real-time performance. In this paper, the current state-of-the-art on real-time object detection/tracking is overviewed. Additionally, a relevant, modular implementation suitable for on-drone execution (running on top of the popular Robot Operating System) is presented and empirically evaluated on a number of relevant datasets. The results indicate that a sophisticated, neural network-based detection and tracking system can be deployed at real-time even on embedded devices.},
	pages = {7},
	author = {Nousi, Paraskevi and Mademlis, Ioannis and Karakostas, Iason and Tefas, Anastasios and Pitas, Ioannis},
	langid = {english},
	file = {Nousi et al. - EMBEDDED UAV REAL-TIME VISUAL OBJECT DETECTION AND.pdf:/home/ritzo/Zotero/storage/JD8DPL97/Nousi et al. - EMBEDDED UAV REAL-TIME VISUAL OBJECT DETECTION AND.pdf:application/pdf}
}

@article{hanhirova_latency_2018-1,
	title = {Latency and Throughput Characterization of Convolutional Neural Networks for Mobile Computer Vision},
	url = {http://arxiv.org/abs/1803.09492},
	abstract = {We study performance characteristics of convolutional neural networks ({CNN}) for mobile computer vision systems. {CNNs} have proven to be a powerful and efficient approach to implement such systems. However, the system performance depends largely on the utilization of hardware accelerators, which are able to speed up the execution of the underlying mathematical operations tremendously through massive parallelism. Our contribution is performance characterization of multiple {CNN}-based models for object recognition and detection with several different hardware platforms and software frameworks, using both local (on-device) and remote (network-side server) computation. The measurements are conducted using real workloads and real processing platforms. On the platform side, we concentrate especially on {TensorFlow} and {TensorRT}. Our measurements include embedded processors found on mobile devices and high-performance processors that can be used on the network side of mobile systems. We show that there exists significant latency--throughput trade-offs but the behavior is very complex. We demonstrate and discuss several factors that affect the performance and yield this complex behavior.},
	journaltitle = {{arXiv}:1803.09492 [cs]},
	author = {Hanhirova, Jussi and Kämäräinen, Teemu and Seppälä, Sipi and Siekkinen, Matti and Hirvisalo, Vesa and Ylä-Jääski, Antti},
	urldate = {2019-11-05},
	date = {2018-03-26},
	eprinttype = {arxiv},
	eprint = {1803.09492},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/ZKY6S3YK/Hanhirova et al. - 2018 - Latency and Throughput Characterization of Convolu.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/LWXS3P62/1803.html:text/html}
}

@article{tran_learning_2015,
	title = {Learning Spatiotemporal Features with 3D Convolutional Networks},
	url = {http://arxiv.org/abs/1412.0767},
	abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D {ConvNets}) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D {ConvNets} are more suitable for spatiotemporal feature learning compared to 2D {ConvNets}; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D {ConvNets}; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on {UCF}101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of {ConvNets}. Finally, they are conceptually very simple and easy to train and use.},
	journaltitle = {{arXiv}:1412.0767 [cs]},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	urldate = {2019-11-05},
	date = {2015-10-06},
	eprinttype = {arxiv},
	eprint = {1412.0767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/H9SZJDB6/Tran et al. - 2015 - Learning Spatiotemporal Features with 3D Convoluti.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/BLVHEMD6/1412.html:text/html}
}

@article{li_tracknet:_2019,
	title = {{TrackNet}: Simultaneous Object Detection and Tracking and Its Application in Traffic Video Analysis},
	url = {http://arxiv.org/abs/1902.01466},
	shorttitle = {{TrackNet}},
	abstract = {Object detection and object tracking are usually treated as two separate processes. Significant progress has been made for object detection in 2D images using deep learning networks. The usual tracking-by-detection pipeline for object tracking requires that the object is successfully detected in the first frame and all subsequent frames, and tracking is done by associating detection results. Performing object detection and object tracking through a single network remains a challenging open question. We propose a novel network structure named {trackNet} that can directly detect a 3D tube enclosing a moving object in a video segment by extending the faster R-{CNN} framework. A Tube Proposal Network ({TPN}) inside the {trackNet} is proposed to predict the objectness of each candidate tube and location parameters specifying the bounding tube. The proposed framework is applicable for detecting and tracking any object and in this paper, we focus on its application for traffic video analysis. The proposed model is trained and tested on {UA}-{DETRAC}, a large traffic video dataset available for multi-vehicle detection and tracking, and obtained very promising results.},
	journaltitle = {{arXiv}:1902.01466 [cs]},
	author = {Li, Chenge and Dobler, Gregory and Feng, Xin and Wang, Yao},
	urldate = {2019-11-05},
	date = {2019-02-04},
	eprinttype = {arxiv},
	eprint = {1902.01466},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/B9SFH8C8/Li et al. - 2019 - TrackNet Simultaneous Object Detection and Tracki.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/DKZ8E5YB/1902.html:text/html}
}

@article{li_deep_nodate,
	title = {{DEEP} {LEARNING} {FOR} {OBJECT} {DETECTION} {AND} {TRACKING} {AND} {FOR} {FIELD} {OF} {VIEW} {PREDICTION} {IN} 360-{DEGREE} {VIDEOS}},
	pages = {140},
	author = {Li, Chenge},
	langid = {english}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2019-11-05},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/MX4D2T3M/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/LTVGZJG3/1512.html:text/html}
}

@inproceedings{belhassen_improving_2019,
	location = {Prague, Czech Republic},
	title = {Improving Video Object Detection by Seq-Bbox Matching:},
	isbn = {978-989-758-354-4},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0007260002260233},
	doi = {10.5220/0007260002260233},
	shorttitle = {Improving Video Object Detection by Seq-Bbox Matching},
	eventtitle = {14th International Conference on Computer Vision Theory and Applications},
	pages = {226--233},
	booktitle = {Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Belhassen, Hatem and Zhang, Heng and Fresse, Virginie and Bourennane, El-Bay},
	urldate = {2019-11-05},
	date = {2019},
	langid = {english},
	file = {VISAPP_2019_32_CR.pdf:/home/ritzo/Downloads/VISAPP_2019_32_CR.pdf:application/pdf}
}

@article{kim_hybrid_2018,
	title = {A hybrid framework combining background subtraction and deep neural networks for rapid person detection},
	volume = {5},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-018-0131-x},
	doi = {10.1186/s40537-018-0131-x},
	abstract = {Currently, the number of surveillance cameras is rapidly increasing responding to secu‑rity issues. But constructing an intelligent detection system is not easy because it needs high computing performance. This study aims to construct a real-world video surveil‑lance system that can effectively detect moving person using limited resources. To this end, we propose a simple framework to detect and recognize moving objects using outdoor {CCTV} video footages by combining background subtraction and Convolu‑tional Neural Networks ({CNNs}). A background subtraction algorithm is first applied to each video frame to find the regions of interest ({ROIs}). A {CNN} classification is then car‑ried out to classify the obtained {ROIs} into one of the predefined classes. Our approach much reduces the computation complexity in comparison to other object detection algorithms. For the experiments, new datasets are constructed by filming alleys and playgrounds, places where crimes are likely to occur. Different image sizes and experi‑mental settings are tested to construct the best classifier for detecting people. The best classification accuracy of 0.85 was obtained for a test set from the same camera with training set and 0.82 with different cameras.},
	pages = {22},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Kim, Chulyeon and Lee, Jiyoung and Han, Taekjin and Kim, Young-Min},
	urldate = {2019-11-06},
	date = {2018-12},
	langid = {english},
	file = {Kim et al. - 2018 - A hybrid framework combining background subtractio.pdf:/home/ritzo/Zotero/storage/8YGJMXLF/Kim et al. - 2018 - A hybrid framework combining background subtractio.pdf:application/pdf}
}

@article{wojke_simple_2017,
	title = {Simple Online and Realtime Tracking with a Deep Association Metric},
	url = {http://arxiv.org/abs/1703.07402},
	abstract = {Simple Online and Realtime Tracking ({SORT}) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of {SORT}. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45\%, achieving overall competitive performance at high frame rates.},
	journaltitle = {{arXiv}:1703.07402 [cs]},
	author = {Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
	urldate = {2019-11-06},
	date = {2017-03-21},
	eprinttype = {arxiv},
	eprint = {1703.07402},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/82UW8EYT/Wojke et al. - 2017 - Simple Online and Realtime Tracking with a Deep As.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/XDJIKPWU/1703.html:text/html}
}

@inproceedings{cheung_robust_2004,
	location = {San Jose, {CA}},
	title = {Robust techniques for background subtraction in urban traffic video},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.526886},
	doi = {10.1117/12.526886},
	abstract = {Identifying moving objects from a video sequence is a fundamental and critical task in many computer-vision applications. A common approach is to perform background subtraction, which identiﬁes moving objects from the portion of a video frame that diﬀers signiﬁcantly from a background model. There are many challenges in developing a good background subtraction algorithm. First, it must be robust against changes in illumination. Second, it should avoid detecting non-stationary background objects such as swinging leaves, rain, snow, and shadow cast by moving objects. Finally, its internal background model should react quickly to changes in background such as starting and stopping of vehicles. In this paper, we compare various background subtraction algorithms for detecting moving vehicles and pedestrians in urban traﬃc video sequences. We consider approaches varying from simple techniques such as frame diﬀerencing and adaptive median ﬁltering, to more sophisticated probabilistic modeling techniques. While complicated techniques often produce superior performance, our experiments show that simple techniques such as adaptive median ﬁltering can produce good results with much lower computational complexity.},
	eventtitle = {Electronic Imaging 2004},
	pages = {881},
	author = {Cheung, Sen-ching S. and Kamath, Chandrika},
	editor = {Panchanathan, Sethuraman and Vasudev, Bhaskaran},
	urldate = {2019-11-07},
	date = {2004-01-07},
	langid = {english},
	file = {Cheung and Kamath - 2004 - Robust techniques for background subtraction in ur.pdf:/home/ritzo/Zotero/storage/7T8HCMHS/Cheung and Kamath - 2004 - Robust techniques for background subtraction in ur.pdf:application/pdf}
}

@article{santoyo-morales_video_2014,
	title = {Video Background Subtraction in Complex Environments},
	volume = {12},
	issn = {16656423},
	url = {http://www.jart.icat.unam.mx/index.php/jart/article/view/213},
	doi = {10.1016/S1665-6423(14)71632-3},
	abstract = {Background subtraction models based on mixture of Gaussians have been extensively used for detecting objects in motion in a wide variety of computer vision applications. However, background subtraction modeling is still an open problem particularly in video scenes with drastic illumination changes and dynamic backgrounds (complex backgrounds). The purpose of the present work is focused on increasing the robustness of background subtraction models to complex environments. For this, we proposed the following enhancements: a) redefine the model distribution parameters involved in the detection of moving objects (distribution weight, mean and variance), b) improve pixel classification (background/foreground) and variable update mechanism by a new time-space dependent learning-rate parameter, and c) replace the pixel-based modeling currently used in the literature by a new space-time region-based model that eliminates the noise effect caused by drastic changes in illumination. Our proposed scheme can be implemented on any state of the art background subtraction scheme based on mixture of Gaussians to improve its resilient to complex backgrounds. Experimental results show excellent noise removal and object motion detection properties under complex environments.},
	pages = {527--537},
	number = {3},
	journaltitle = {Journal of Applied Research and Technology},
	shortjournal = {Journal of Applied Research and Technology},
	author = {Santoyo-Morales, Juana E. and Hasimoto-Beltran, Rogelio},
	urldate = {2019-11-07},
	date = {2014-06},
	langid = {english},
	file = {Santoyo-Morales and Hasimoto-Beltran - 2014 - Video Background Subtraction in Complex Environmen.pdf:/home/ritzo/Zotero/storage/HMC2FHYN/Santoyo-Morales and Hasimoto-Beltran - 2014 - Video Background Subtraction in Complex Environmen.pdf:application/pdf}
}

@article{yao_comparative_2017,
	title = {Comparative Evaluation of Background Subtraction Algorithms in Remote Scene Videos Captured by {MWIR} Sensors},
	volume = {17},
	issn = {1424-8220},
	url = {http://www.mdpi.com/1424-8220/17/9/1945},
	doi = {10.3390/s17091945},
	abstract = {Background subtraction ({BS}) is one of the most commonly encountered tasks in video analysis and tracking systems. It distinguishes the foreground (moving objects) from the video sequences captured by static imaging sensors. Background subtraction in remote scene infrared ({IR}) video is important and common to lots of ﬁelds. This paper provides a Remote Scene {IR} Dataset captured by our designed medium-wave infrared ({MWIR}) sensor. Each video sequence in this dataset is identiﬁed with speciﬁc {BS} challenges and the pixel-wise ground truth of foreground ({FG}) for each frame is also provided. A series of experiments were conducted to evaluate {BS} algorithms on this proposed dataset. The overall performance of {BS} algorithms and the processor/memory requirements were compared. Proper evaluation metrics or criteria were employed to evaluate the capability of each {BS} algorithm to handle different kinds of {BS} challenges represented in this dataset. The results and conclusions in this paper provide valid references to develop new {BS} algorithm for remote scene {IR} video sequence, and some of them are not only limited to remote scene or {IR} video sequence but also generic for background subtraction. The Remote Scene {IR} dataset and the foreground masks detected by each evaluated {BS} algorithm are available online: https://github.com/{JerryYaoGl}/{BSEvaluationRemoteSceneIR}.},
	pages = {1945},
	number = {9},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Yao, Guangle and Lei, Tao and Zhong, Jiandan and Jiang, Ping and Jia, Wenwu},
	urldate = {2019-11-07},
	date = {2017-08-24},
	langid = {english},
	file = {Yao et al. - 2017 - Comparative Evaluation of Background Subtraction A.pdf:/home/ritzo/Zotero/storage/EMGJ7AM7/Yao et al. - 2017 - Comparative Evaluation of Background Subtraction A.pdf:application/pdf}
}

@article{jeeva_twin_2017,
	title = {Twin background model for foreground detection in video sequence},
	issn = {1386-7857, 1573-7543},
	url = {http://link.springer.com/10.1007/s10586-017-1446-7},
	doi = {10.1007/s10586-017-1446-7},
	abstract = {The realm of video surveillance has various methods to extract foreground. Background subtraction is one of the prime methods for automatic video analysis. The sensitivity of a meaningful event of interest is increased due to dampening effect of background changes and detection of false alarm. Hence the model can be strongly recommended to industries. This paper restricts the focus to one of the most common causes of dynamic background changes: that of swaying trees branches and illumination changes. To overcome the issue available in existing system, we propose a method called as Twin Background Modeling ({TBGM}). This method has dual models namely long and short term background models to increase exposure rate of foreground by using statistical method and also reduce false negative rate. This method has dimensional transformation from 2D to 1D which reduces computation time of the system and increases batch processing. The proposed method uses Manhattan distance to reduce execution time, increase detection rate and reduce error rate. The performance of the suggested approach is illustrated by using change detection dataset 2014 and is compared to other conventional approaches.},
	journaltitle = {Cluster Computing},
	shortjournal = {Cluster Comput},
	author = {Jeeva, S. and Sivabalakrishnan, M.},
	urldate = {2019-11-11},
	date = {2017-12-07},
	langid = {english},
	file = {Jeeva and Sivabalakrishnan - 2017 - Twin background model for foreground detection in .pdf:/home/ritzo/Zotero/storage/I6ZWIKML/Jeeva and Sivabalakrishnan - 2017 - Twin background model for foreground detection in .pdf:application/pdf}
}

@article{erokhin_detection_2017,
	title = {{DETECTION} {AND} {TRACKING} {OF} {MOVING} {OBJECTS} {WITH} {REAL}-{TIME} {ONBOARD} {VISION} {SYSTEM}},
	volume = {{XLII}-2/W4},
	issn = {2194-9034},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-2-W4/67/2017/},
	doi = {10.5194/isprs-archives-XLII-2-W4-67-2017},
	abstract = {Detection of moving objects in video sequence received from moving video sensor is a one of the most important problem in computer vision. The main purpose of this work is developing set of algorithms, which can detect and track moving objects in real time computer vision system. This set includes three main parts: the algorithm for estimation and compensation of geometric transformations of images, an algorithm for detection of moving objects, an algorithm to tracking of the detected objects and prediction their position. The results can be claimed to create onboard vision systems of aircraft, including those relating to small and unmanned aircraft.},
	pages = {67--71},
	journaltitle = {{ISPRS} - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	shortjournal = {Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci.},
	author = {Erokhin, D. Y. and Feldman, A. B. and Korepanov, S. E.},
	urldate = {2019-11-11},
	date = {2017-05-10},
	langid = {english},
	file = {Erokhin et al. - 2017 - DETECTION AND TRACKING OF MOVING OBJECTS WITH REAL.pdf:/home/ritzo/Zotero/storage/QN6PNTD5/Erokhin et al. - 2017 - DETECTION AND TRACKING OF MOVING OBJECTS WITH REAL.pdf:application/pdf}
}

@article{yu_combining_2019,
	title = {Combining Background Subtraction and Convolutional Neural Network for Anomaly Detection in Pumping-Unit Surveillance},
	volume = {12},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/12/6/115},
	doi = {10.3390/a12060115},
	abstract = {Background subtraction plays a fundamental role for anomaly detection in video surveillance, which is able to tell where moving objects are in the video scene. Regrettably, the regular rotating pumping unit is treated as an abnormal object by the background-subtraction method in pumping-unit surveillance. As an excellent classiﬁer, a deep convolutional neural network is able to tell what those objects are. Therefore, we combined background subtraction and a convolutional neural network to perform anomaly detection for pumping-unit surveillance. In the proposed method, background subtraction was applied to ﬁrst extract moving objects. Then, a clustering method was adopted for extracting diﬀerent object types that had more movement-foreground objects but fewer typical targets. Finally, nonpumping unit objects were identiﬁed as abnormal objects by the trained classiﬁcation network. The experimental results demonstrate that the proposed method can detect abnormal objects in a pumping-unit scene with high accuracy.},
	pages = {115},
	number = {6},
	journaltitle = {Algorithms},
	shortjournal = {Algorithms},
	author = {Yu, Tianming and Yang, Jianhua and Lu, Wei},
	urldate = {2019-11-11},
	date = {2019-05-29},
	langid = {english},
	file = {Yu et al. - 2019 - Combining Background Subtraction and Convolutional.pdf:/home/ritzo/Zotero/storage/7IY7KW2H/Yu et al. - 2019 - Combining Background Subtraction and Convolutional.pdf:application/pdf}
}

@article{sobral_comprehensive_2014,
	title = {A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos},
	volume = {122},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314213002361},
	doi = {10.1016/j.cviu.2013.12.005},
	abstract = {Background subtraction ({BS}) is a crucial step in many computer vision systems, as it is ﬁrst applied to detect moving objects within a video stream. Many algorithms have been designed to segment the foreground objects from the background of a sequence. In this article, we propose to use the {BMC} (Background Models Challenge) dataset, and to compare the 29 methods implemented in the {BGSLibrary}. From this large set of various {BG} methods, we have conducted a relevant experimental analysis to evaluate both their robustness and their practical performance in terms of processor/ memory requirements.},
	pages = {4--21},
	journaltitle = {Computer Vision and Image Understanding},
	shortjournal = {Computer Vision and Image Understanding},
	author = {Sobral, Andrews and Vacavant, Antoine},
	urldate = {2019-11-11},
	date = {2014-05},
	langid = {english},
	file = {Sobral and Vacavant - 2014 - A comprehensive review of background subtraction a.pdf:/home/ritzo/Zotero/storage/LM93N7RW/Sobral and Vacavant - 2014 - A comprehensive review of background subtraction a.pdf:application/pdf}
}

@article{jaderberg_population_2017,
	title = {Population Based Training of Neural Networks},
	url = {http://arxiv.org/abs/1711.09846},
	abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present {\textbackslash}emph\{Population Based Training ({PBT})\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, {PBT} discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of {PBT} on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where {PBT} is used to maximise the {BLEU} score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases {PBT} results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
	journaltitle = {{arXiv}:1711.09846 [cs]},
	author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
	urldate = {2019-11-12},
	date = {2017-11-28},
	eprinttype = {arxiv},
	eprint = {1711.09846},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/TJRHQG9T/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf:application/pdf}
}

@incollection{alexandre_bmog:_2017,
	location = {Cham},
	title = {{BMOG}: Boosted Gaussian Mixture Model with Controlled Complexity},
	volume = {10255},
	isbn = {978-3-319-58837-7 978-3-319-58838-4},
	url = {http://link.springer.com/10.1007/978-3-319-58838-4_6},
	shorttitle = {{BMOG}},
	abstract = {Developing robust and universal methods for unsupervised segmentation of moving objects in video sequences has proved to be a hard and challenging task. The best solutions are, in general, computationally heavy preventing their use in real-time applications. This research addresses this problem by proposing a robust and computationally eﬃcient method, {BMOG}, that signiﬁcantly boosts the performance of the widely used {MOG}2 method. The complexity of {BMOG} is kept low, proving its suitability for real-time applications. The proposed solution explores a novel classiﬁcation mechanism that combines color space discrimination capabilities with hysteresis and a dynamic learning rate for background model update.},
	pages = {50--57},
	booktitle = {Pattern Recognition and Image Analysis},
	publisher = {Springer International Publishing},
	author = {Martins, Isabel and Carvalho, Pedro and Corte-Real, Luís and Alba-Castro, José Luis},
	editor = {Alexandre, Luís A. and Salvador Sánchez, José and Rodrigues, João M. F.},
	urldate = {2019-11-13},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-58838-4_6},
	file = {P-00N-9XM.pdf:/home/ritzo/Downloads/P-00N-9XM.pdf:application/pdf}
}

@article{lobina_konzeption_nodate,
	title = {Konzeption und Evaluierung eines Ansatzes zum Webscraping mittels natürlicher Befehle am Beispiel von Check24.de},
	pages = {52},
	author = {Lobina, Alessandro and Darmstadt, Hochschule and Informatik, Fachbereich},
	langid = {german},
	file = {2019-03-08_Bachelorarbeit-Lobina.pdf:/home/ritzo/Downloads/2019-03-08_Bachelorarbeit-Lobina.pdf:application/pdf}
}

@inproceedings{lokhmotov_multi-objective_2018,
	location = {Williamsburg, {VA}, {USA}},
	title = {Multi-objective autotuning of {MobileNets} across the full software/hardware stack},
	isbn = {978-1-4503-5923-8},
	url = {http://dl.acm.org/citation.cfm?doid=3229762.3229767},
	doi = {10.1145/3229762.3229767},
	abstract = {We present a customizable Collective Knowledge workflow to study the execution time vs. accuracy trade-offs for the {MobileNets} {CNN} family. We use this workflow to evaluate {MobileNets} on Arm Cortex {CPUs} using {TensorFlow} and Arm Mali {GPUs} using several versions of the Arm Compute Library. Our optimizations for the Arm Bifrost {GPU} architecture reduce the execution time by 2–3 times, while lying on a Pareto-optimal frontier. We also highlight the challenge of maintaining the accuracy when deploying {CNN} models across diverse platforms. We make all the workflow components (models, programs, scripts, etc.) publicly available to encourage further exploration by the community.},
	eventtitle = {the 1st},
	pages = {1},
	booktitle = {Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-designing Pareto-efficient Deep Learning  - {ReQuEST} '18},
	publisher = {{ACM} Press},
	author = {Lokhmotov, Anton and Chunosov, Nikolay and Vella, Flavio and Fursin, Grigori},
	urldate = {2019-11-14},
	date = {2018},
	langid = {english},
	file = {Multi-objective_autotuning_of_MobileNets_across_th.pdf:/home/ritzo/Downloads/Multi-objective_autotuning_of_MobileNets_across_th.pdf:application/pdf}
}

@article{chauhan_embedded_2019,
	title = {Embedded {CNN} based vehicle classification and counting in non-laned road traffic},
	url = {http://arxiv.org/abs/1901.06358},
	abstract = {Classifying and counting vehicles in road traffic has numerous applications in the transportation engineering domain. However, the wide variety of vehicles (two-wheelers, three-wheelers, cars, buses, trucks etc.) plying on roads of developing regions without any lane discipline, makes vehicle classification and counting a hard problem to automate. In this paper, we use state of the art Convolutional Neural Network ({CNN}) based object detection models and train them for multiple vehicle classes using data from Delhi roads. We get upto 75\% {MAP} on an 80-20 train-test split using 5562 video frames from four different locations. As robust network connectivity is scarce in developing regions for continuous video transmissions from the road to cloud servers, we also evaluate the latency, energy and hardware cost of embedded implementations of our {CNN} model based inferences.},
	journaltitle = {{arXiv}:1901.06358 [cs]},
	author = {Chauhan, Mayank Singh and Singh, Arshdeep and Khemka, Mansi and Prateek, Arneish and Sen, Rijurekha},
	urldate = {2019-11-14},
	date = {2019-01-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.06358},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {1901.06358.pdf:/home/ritzo/Downloads/1901.06358.pdf:application/pdf}
}

@article{sandler_mobilenetv2:_2019,
	title = {{MobileNetV}2: Inverted Residuals and Linear Bottlenecks},
	url = {http://arxiv.org/abs/1801.04381},
	shorttitle = {{MobileNetV}2},
	abstract = {In this paper we describe a new mobile architecture, {MobileNetV}2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call {SSDLite}. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of {DeepLabv}3 which we call Mobile {DeepLabv}3. The {MobileNetV}2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an {MobileNetV}2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, {COCO} object detection, {VOC} image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds ({MAdd}), as well as the number of parameters},
	journaltitle = {{arXiv}:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	urldate = {2019-11-14},
	date = {2019-03-21},
	eprinttype = {arxiv},
	eprint = {1801.04381},
	note = {version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/QA545NSE/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/QIYV7SCY/1801.html:text/html}
}

@article{donahue_decaf:_2013,
	title = {{DeCAF}: A Deep Convolutional Activation Feature for Generic Visual Recognition},
	url = {http://arxiv.org/abs/1310.1531},
	shorttitle = {{DeCAF}},
	abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing {DeCAF}, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
	journaltitle = {{arXiv}:1310.1531 [cs]},
	author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
	urldate = {2019-11-14},
	date = {2013-10-05},
	eprinttype = {arxiv},
	eprint = {1310.1531},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/G3NZGUTG/Donahue et al. - 2013 - DeCAF A Deep Convolutional Activation Feature for.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/USKNC5US/1310.html:text/html}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2019-11-14},
	date = {2015-05},
	langid = {english},
	file = {DeepLearning.pdf:/home/ritzo/Downloads/DeepLearning.pdf:application/pdf}
}

@article{ren_faster_2016,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	url = {http://arxiv.org/abs/1506.01497},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} and Fast R-{CNN} have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network ({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The {RPN} is trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. We further merge {RPN} and Fast R-{CNN} into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the {RPN} component tells the unified network where to look. For the very deep {VGG}-16 model, our detection system has a frame rate of 5fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007, 2012, and {MS} {COCO} datasets with only 300 proposals per image. In {ILSVRC} and {COCO} 2015 competitions, Faster R-{CNN} and {RPN} are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	journaltitle = {{arXiv}:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	urldate = {2019-11-14},
	date = {2016-01-06},
	eprinttype = {arxiv},
	eprint = {1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/5C8L65ZH/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/RL5C6T9R/1506.html:text/html}
}

@inproceedings{toyama_wallflower:_1999,
	location = {Kerkyra, Greece},
	title = {Wallflower: principles and practice of background maintenance},
	isbn = {978-0-7695-0164-2},
	url = {http://ieeexplore.ieee.org/document/791228/},
	doi = {10.1109/ICCV.1999.791228},
	shorttitle = {Wallflower},
	abstract = {Background maintenance is a frequent element of video surveillance systems. We develop Wallflower, a threecomponent system for background maintenance: the pixellevel component performs Wiener filtering to make probabilistic predictions of the expected background; the region-level component fills in homogeneous regions of foreground objects; and the frame-level component detects sudden, global changes in the image and swaps in better approximations of the background.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {255--261 vol.1},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Toyama, K. and Krumm, J. and Brumitt, B. and Meyers, B.},
	urldate = {2019-11-14},
	date = {1999},
	langid = {english},
	file = {toyama_ICCV_99.pdf:/home/ritzo/Downloads/toyama_ICCV_99.pdf:application/pdf}
}

@article{howard_searching_2019,
	title = {Searching for {MobileNetV}3},
	url = {http://arxiv.org/abs/1905.02244},
	abstract = {We present the next generation of {MobileNets} based on a combination of complementary search techniques as well as a novel architecture design. {MobileNetV}3 is tuned to mobile phone {CPUs} through a combination of hardware-aware network architecture search ({NAS}) complemented by the {NetAdapt} algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new {MobileNet} models for release: {MobileNetV}3-Large and {MobileNetV}3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling ({LR}-{ASPP}). We achieve new state of the art results for mobile classification, detection and segmentation. {MobileNetV}3-Large is 3.2{\textbackslash}\% more accurate on {ImageNet} classification while reducing latency by 15{\textbackslash}\% compared to {MobileNetV}2. {MobileNetV}3-Small is 4.6{\textbackslash}\% more accurate while reducing latency by 5{\textbackslash}\% compared to {MobileNetV}2. {MobileNetV}3-Large detection is 25{\textbackslash}\% faster at roughly the same accuracy as {MobileNetV}2 on {COCO} detection. {MobileNetV}3-Large {LR}-{ASPP} is 30{\textbackslash}\% faster than {MobileNetV}2 R-{ASPP} at similar accuracy for Cityscapes segmentation.},
	journaltitle = {{arXiv}:1905.02244 [cs]},
	author = {Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and Le, Quoc V. and Adam, Hartwig},
	urldate = {2019-11-14},
	date = {2019-08-24},
	eprinttype = {arxiv},
	eprint = {1905.02244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/DJRR9G9X/Howard et al. - 2019 - Searching for MobileNetV3.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/HSWWXCUL/1905.html:text/html}
}

@article{st-charles_subsense:_2015,
	title = {{SuBSENSE}: A Universal Change Detection Method With Local Adaptive Sensitivity},
	volume = {24},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/6975239/},
	doi = {10.1109/TIP.2014.2378053},
	shorttitle = {{SuBSENSE}},
	abstract = {Foreground/background segmentation via change detection in video sequences is often used as a stepping stone in high-level analytics and applications. Despite the wide variety of methods that have been proposed for this problem, none has been able to fully address the complex nature of dynamic scenes in real surveillance tasks. In this paper, we present a universal pixellevel segmentation method that relies on spatiotemporal binary features as well as color information to detect changes. This allows camouﬂaged foreground objects to be detected more easily while most illumination variations are ignored. Besides, instead of using manually-set, frame-wide constants to dictate model sensitivity and adaptation speed, we use pixel-level feedback loops to dynamically adjust our method’s internal parameters without user intervention. These adjustments are based on the continuous monitoring of model ﬁdelity and local segmentation noise levels. This new approach enables us to outperform all 32 previously tested state-of-the-art methods on the 2012 and 2014 versions of the {ChangeDetection}.net dataset in terms of overall {FMeasure}. The use of local binary image descriptors for pixel-level modeling also facilitates high-speed parallel implementations: our own version which used no low-level or architecture-speciﬁc instruction reached real-time processing speed on a mid-level desktop {CPU}. A complete C++ implementation based on {OpenCV} is available online.},
	pages = {359--373},
	number = {1},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {St-Charles, Pierre-Luc and Bilodeau, Guillaume-Alexandre and Bergevin, Robert},
	urldate = {2019-11-14},
	date = {2015-01},
	langid = {english},
	file = {StCharlesetalIEEETIP2015.pdf:/home/ritzo/Downloads/StCharlesetalIEEETIP2015.pdf:application/pdf}
}

@inproceedings{sobral_bgslibrary:_2013,
	location = {Rio de Janeiro, Brazil},
	title = {{BGSLibrary}: An {OpenCV} C++ Background Subtraction Library},
	url = {https://github.com/andrewssobral/bgslibrary},
	booktitle = {{IX} Workshop de Visão Computacional ({WVC}'2013)},
	author = {Sobral, Andrews},
	date = {2013-06}
}

@article{st-charles_subsense:_2015-1,
	title = {{SuBSENSE}: A Universal Change Detection Method With Local Adaptive Sensitivity},
	volume = {24},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/6975239/},
	doi = {10.1109/TIP.2014.2378053},
	shorttitle = {{SuBSENSE}},
	abstract = {Foreground/background segmentation via change detection in video sequences is often used as a stepping stone in high-level analytics and applications. Despite the wide variety of methods that have been proposed for this problem, none has been able to fully address the complex nature of dynamic scenes in real surveillance tasks. In this paper, we present a universal pixellevel segmentation method that relies on spatiotemporal binary features as well as color information to detect changes. This allows camouﬂaged foreground objects to be detected more easily while most illumination variations are ignored. Besides, instead of using manually-set, frame-wide constants to dictate model sensitivity and adaptation speed, we use pixel-level feedback loops to dynamically adjust our method’s internal parameters without user intervention. These adjustments are based on the continuous monitoring of model ﬁdelity and local segmentation noise levels. This new approach enables us to outperform all 32 previously tested state-of-the-art methods on the 2012 and 2014 versions of the {ChangeDetection}.net dataset in terms of overall {FMeasure}. The use of local binary image descriptors for pixel-level modeling also facilitates high-speed parallel implementations: our own version which used no low-level or architecture-speciﬁc instruction reached real-time processing speed on a mid-level desktop {CPU}. A complete C++ implementation based on {OpenCV} is available online.},
	pages = {359--373},
	number = {1},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {St-Charles, Pierre-Luc and Bilodeau, Guillaume-Alexandre and Bergevin, Robert},
	urldate = {2019-11-15},
	date = {2015-01},
	langid = {english},
	file = {StCharlesetalIEEETIP2015.pdf:C\:\\Users\\klein\\Downloads\\StCharlesetalIEEETIP2015.pdf:application/pdf}
}

@online{noauthor_change_nodate,
	title = {Change detection benchmark web site},
	url = {http://jacarini.dinf.usherbrooke.ca/cdw2014/},
	urldate = {2019-11-15},
	file = {Change detection benchmark web site:/home/ritzo/Zotero/storage/ZGYLSDHB/cdw2014.html:text/html}
}

@inproceedings{zivkovic_improved_2004,
	location = {Cambridge, {UK}},
	title = {Improved adaptive Gaussian mixture model for background subtraction},
	isbn = {978-0-7695-2128-2},
	url = {http://ieeexplore.ieee.org/document/1333992/},
	doi = {10.1109/ICPR.2004.1333992},
	abstract = {Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We develop an efﬁcient adaptive algorithm using Gaussian mixture probability density. Recursive equations are used to constantly update the parameters and but also to simultaneously select the appropriate number of components for each pixel.},
	eventtitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. {ICPR} 2004.},
	pages = {28--31 Vol.2},
	booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. {ICPR} 2004.},
	publisher = {{IEEE}},
	author = {Zivkovic, Z.},
	urldate = {2019-11-15},
	date = {2004},
	langid = {english},
	file = {Zivkovic - 2004 - Improved adaptive Gaussian mixture model for backg.pdf:/home/ritzo/Zotero/storage/YDXXWARK/Zivkovic - 2004 - Improved adaptive Gaussian mixture model for backg.pdf:application/pdf}
}

@inproceedings{kim_simultaneous_2015,
	location = {Santiago},
	title = {Simultaneous Foreground Detection and Classification with Hybrid Features},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410735/},
	doi = {10.1109/ICCV.2015.378},
	abstract = {In this paper, we propose a hybrid background model that relies on edge and non-edge features of the image to produce the model. We encode these features into a coding scheme, that we called Local Hybrid Pattern ({LHP}), that selectively models edges and non-edges features of each pixel. Furthermore, we model each pixel with an adaptive code dictionary to represent the background dynamism, and update it by adding stable codes and discarding unstable ones. We weight each code in the dictionary to enhance its description of the pixel it models. The foreground is detected as the incoming codes that deviate from the dictionary. We can detect (as foreground or background) and classify (as edge or inner region) each pixel simultaneously. We tested our proposed method in existing databases with promising results.},
	eventtitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {3307--3315},
	booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	publisher = {{IEEE}},
	author = {Kim, Jaemyun and Rivera, Adin Ramirez and Ryu, Byungyong and Chae, Oksam},
	urldate = {2019-11-18},
	date = {2015-12},
	langid = {english},
	file = {Kim_Simultaneous_Foreground_Detection_ICCV_2015_paper.pdf:/home/ritzo/Downloads/Kim_Simultaneous_Foreground_Detection_ICCV_2015_paper.pdf:application/pdf}
}

@inproceedings{goyette_changedetection.net:_2012,
	location = {Providence, {RI}, {USA}},
	title = {Changedetection.net: A new change detection benchmark dataset},
	isbn = {978-1-4673-1612-5 978-1-4673-1611-8 978-1-4673-1610-1},
	url = {http://ieeexplore.ieee.org/document/6238919/},
	doi = {10.1109/CVPRW.2012.6238919},
	shorttitle = {Changedetection.net},
	abstract = {Change detection is one of the most commonly encountered low-level tasks in computer vision and video processing. A plethora of algorithms have been developed to date, yet no widely accepted, realistic, large-scale video dataset exists for benchmarking diﬀerent methods. Presented here is a unique change detection benchmark dataset consisting of nearly 90,000 frames in 31 video sequences representing 6 categories selected to cover a wide range of challenges in 2 modalities (color and thermal {IR}). A distinguishing characteristic of this dataset is that each frame is meticulously annotated for ground-truth foreground, background, and shadow area boundaries - an eﬀort that goes much beyond a simple binary label denoting the presence of change. This enables objective and precise quantitative comparison and ranking of change detection algorithms. This paper presents and discusses various aspects of the new dataset, quantitative performance metrics used, and comparative results for over a dozen previous and new change detection algorithms. The dataset, evaluation tools, and algorithm rankings are available to the public on a website1 and will be updated with feedback from academia and industry in the future.},
	eventtitle = {2012 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops ({CVPR} Workshops)},
	pages = {1--8},
	booktitle = {2012 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	publisher = {{IEEE}},
	author = {Goyette, Nil and Jodoin, Pierre-Marc and Porikli, Fatih and Konrad, Janusz and Ishwar, Prakash},
	urldate = {2019-11-19},
	date = {2012-06},
	langid = {english},
	file = {TR2012-044.pdf:/home/ritzo/Downloads/TR2012-044.pdf:application/pdf}
}

@article{hagerstrand_evaluation_nodate,
	title = {Evaluation of Background Subtraction in Pan-Tilt Camera Tracking},
	abstract = {Object tracking is the subﬁeld of computer vision where an object is to be located in each frame of a video sequence. Automated tracking is useful in all areas where vision and cameras are used. Computers can assist in time-consuming tasks in television or surveillance as well as stabilise and increase tracking precision compared to manual operation. In a system using a movable camera such as a pan-tilt-zoom camera mounted on a robot, information about the pan-tilt-zoom conﬁguration can be used to locate a moving object in successive frames since static background can be accounted for from one frame to the next. Two state-of-the-art trackers, called Adaptive Scale Mean Shift ({ASMS}) and Kernel Correlation Filter ({KCF}), as well as a tracker based on Shi-Tomasi corner detection together with optical ﬂow ({OPTFLOW}), are in this thesis evaluated when using a pre-processing stage of online background subtraction based on the historical pixel value distribution. All trackers with and without background subtraction were evaluated for robustness on multiple scenarios containing either a circular unicoloured object or a multicoloured polygon in front of two diﬀerent backgrounds respectively. The tracking performance was shown to not beneﬁt from this particular background subtraction since the amount of wrongly classiﬁed background pixels ruined more than the correctly classiﬁed pixels helped. The implemented background subtraction model aﬀected {OPTFLOW} the most since the background subtraction removed important corner features, while {ASMS} and {KCF} were robust and unaﬀected by the background subtraction. The background subtraction routine for a static camera view was successfully adapted to function for a translating camera, and may be of more use for some trackers not evaluated.},
	pages = {60},
	author = {Hagerstrand, Markus and Karlsson, Hjalmar},
	langid = {english},
	file = {ca395305e06f16243378c7036d3a811c7af5.pdf:/home/ritzo/Downloads/ca395305e06f16243378c7036d3a811c7af5.pdf:application/pdf}
}

@inproceedings{kloss_learning_2016,
	location = {Daejeon, South Korea},
	title = {Learning where to search using visual attention},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759770/},
	doi = {10.1109/IROS.2016.7759770},
	abstract = {Detecting and identifying the diﬀerent objects in an image fast and reliably is an important skill for interacting with one’s environment. The main problem is that in theory, all parts of an image have to be searched for objects on many diﬀerent scales to make sure that no object instance is missed. It however takes considerable time and eﬀort to actually classify the content of a given image region and both time and computational capacities that an agent can spend on classiﬁcation are limited. Humans use a process called visual attention to quickly decide which locations of an image need to be processed in detail and which can be ignored. This allows us to deal with the huge amount of visual information and to employ the capacities of our visual system eﬃciently.},
	eventtitle = {2016 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {5238--5245},
	booktitle = {2016 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	publisher = {{IEEE}},
	author = {Kloss, Alina and Kappler, Daniel and Lensch, Hendrik P. A. and Butz, Martin V. and Schaal, Stefan and Bohg, Jeannette},
	urldate = {2019-11-20},
	date = {2016-10},
	langid = {english},
	file = {thesis.pdf:/home/ritzo/Downloads/thesis.pdf:application/pdf}
}

@incollection{maddalena_background_2012,
	title = {Background Subtraction for Visual Surveillance: A Fuzzy Approach},
	volume = {20123846},
	isbn = {978-1-4398-5684-0 978-1-4398-5685-7},
	url = {http://www.crcnetbase.com/doi/abs/10.1201/b11631-6},
	shorttitle = {Background Subtraction for Visual Surveillance},
	pages = {103--138},
	booktitle = {Handbook on Soft Computing for Video Surveillance},
	publisher = {Chapman and Hall/{CRC}},
	author = {Bouwmans, Thierry},
	editor = {Maddalena, Lucia},
	urldate = {2019-11-20},
	date = {2012-01-25},
	langid = {english},
	doi = {10.1201/b11631-6},
	file = {Background_Subtraction_For_Visual_Survei.pdf:/home/ritzo/Downloads/Background_Subtraction_For_Visual_Survei.pdf:application/pdf}
}

@article{tinz_realisierung_nodate,
	title = {Realisierung eines cloudbasierten Predictive Maintenance Systems mithilfe von statistischer Analyse und maschinellem Lernen für ein physisches Modell},
	pages = {66},
	author = {Tinz, Patrick and Darmstadt, Hochschule and Informatik, Fachbereich},
	langid = {german},
	file = {Thesis_Patrick_Tinz.pdf:/home/ritzo/Downloads/Thesis_Patrick_Tinz.pdf:application/pdf}
}

@article{humm_applied_nodate,
	title = {Applied Artificial Intelligence},
	pages = {46},
	author = {Humm, Dr Bernhard},
	langid = {english},
	file = {Humm - Applied Artificial Intelligence.pdf:/home/ritzo/Zotero/storage/6JNFBH27/Humm - Applied Artificial Intelligence.pdf:application/pdf}
}

@article{humm_applied_nodate-1,
	title = {Applied Artificial Intelligence},
	pages = {68},
	author = {Humm, Dr Bernhard},
	langid = {english},
	file = {02 Machine Learning.pdf:/home/ritzo/Downloads/02 Machine Learning.pdf:application/pdf}
}

@article{kaur_background_nodate,
	title = {Background Subtraction in Video Surveillance},
	pages = {64},
	author = {Kaur, Shanpreet},
	langid = {english},
	file = {Background Subtraction in Video Surveillance.pdf:/home/ritzo/Downloads/Background Subtraction in Video Surveillance.pdf:application/pdf}
}

@article{bradski_opencv_2000,
	title = {The {OpenCV} Library},
	journaltitle = {Dr. Dobb's Journal of Software Tools},
	author = {Bradski, G.},
	date = {2000},
	keywords = {bibtex-import}
}

@article{kern_automatic_nodate-1,
	title = {Automatic Sleep Stage Classiﬁcation using Convolutional Neural Networks with Long Short-Term Memory},
	pages = {48},
	author = {Kern, Simon Johannes},
	langid = {english},
	file = {2_5244822648067720221.pdf:/home/ritzo/Downloads/2_5244822648067720221.pdf:application/pdf}
}

@book{mitchell_machine_1997,
	location = {New York},
	title = {Machine Learning},
	isbn = {978-0-07-042807-2},
	series = {{McGraw}-Hill series in computer science},
	pagetotal = {414},
	publisher = {{McGraw}-Hill},
	author = {Mitchell, Tom M.},
	date = {1997},
	langid = {english},
	keywords = {Computer algorithms, Machine learning},
	file = {McGrawHill - Machine Learning -Tom Mitchell.pdf:/home/ritzo/Downloads/McGrawHill - Machine Learning -Tom Mitchell.pdf:application/pdf}
}

@article{furnkranz_erkennung_nodate,
	title = {Erkennung von Fahrzeugpose und Fahrzeugklasse mit Convolutional Neural Networks},
	pages = {99},
	author = {Fürnkranz, Dr Johannes and Dang, Hien and Luthardt, Stefan},
	langid = {german},
	file = {Muenker_Christoph.pdf:/home/ritzo/Downloads/Muenker_Christoph.pdf:application/pdf}
}

@article{hornik_approximation_1991,
	title = {Approximation capabilities of multilayer feedforward networks},
	volume = {4},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/089360809190009T},
	doi = {10.1016/0893-6080(91)90009-T},
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to V(p.) performance criteria, for arbitrary finite input environment measures Jl., provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	pages = {251--257},
	number = {2},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Hornik, Kurt},
	urldate = {2019-11-21},
	date = {1991},
	langid = {english},
	file = {Hornik-91.pdf:/home/ritzo/Downloads/Hornik-91.pdf:application/pdf}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	pages = {533--536},
	number = {6088},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	date = {1986-10-01},
	file = {naturebp.pdf:/home/ritzo/Downloads/naturebp.pdf:application/pdf}
}

@inproceedings{lavrijsen_high-performance_2016,
	location = {Salt Lake, {UT}, {USA}},
	title = {High-Performance Python-C++ Bindings with {PyPy} and Cling},
	isbn = {978-1-5090-5220-2},
	url = {http://ieeexplore.ieee.org/document/7836841/},
	doi = {10.1109/PyHPC.2016.008},
	eventtitle = {2016 6th Workshop on Python for High-Performance and Scientific Computing ({PyHPC})},
	pages = {27--35},
	booktitle = {2016 6th Workshop on Python for High-Performance and Scientific Computing ({PyHPC})},
	publisher = {{IEEE}},
	author = {Lavrijsen, Wim T.L.P. and Dutta, Aditi},
	urldate = {2019-11-22},
	date = {2016-11},
	langid = {english},
	file = {Lavrijsen and Dutta - 2016 - High-Performance Python-C++ Bindings with PyPy and.pdf:/home/ritzo/Zotero/storage/BMEJNBEA/Lavrijsen and Dutta - 2016 - High-Performance Python-C++ Bindings with PyPy and.pdf:application/pdf}
}

@article{lucks_python_2008,
	title = {Python - All a Scientist Needs},
	url = {http://arxiv.org/abs/0803.1838},
	abstract = {Any cutting-edge scientific research project requires a myriad of computational tools for data generation, management, analysis and visualization. Python is a flexible and extensible scientific programming platform that offered the perfect solution in our recent comparative genomics investigation (J. B. Lucks, D. R. Nelson, G. Kudla, J. B. Plotkin. Genome landscapes and bacteriophage codon usage, {PLoS} Computational Biology, 4, 1000001, 2008). In this paper, we discuss the challenges of this project, and how the combined power of Biopython, Matplotlib and {SWIG} were utilized for the required computational tasks. We finish by discussing how python goes beyond being a convenient programming language, and promotes good scientific practice by enabling clean code, integration with professional programming techniques such as unit testing, and strong data provenance.},
	journaltitle = {{arXiv}:0803.1838 [q-bio]},
	author = {Lucks, Julius B.},
	urldate = {2019-11-22},
	date = {2008-03-12},
	eprinttype = {arxiv},
	eprint = {0803.1838},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/UVRB9BIC/Lucks - 2008 - Python - All a Scientist Needs.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/DUQ9TDZ6/0803.html:text/html}
}

@article{goyal_deep_nodate,
	title = {Deep Learning for Natural Language Processing},
	pages = {290},
	author = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	langid = {english}
}

@incollection{goyal_introduction_2018,
	location = {Berkeley, {CA}},
	title = {Introduction to Natural Language Processing and Deep Learning},
	isbn = {978-1-4842-3685-7},
	url = {https://doi.org/10.1007/978-1-4842-3685-7_1},
	abstract = {Natural language processing ({NPL}) is an extremely difficult task in computer science. Languages present a wide variety of problems that vary from language to language. Structuring or extracting meaningful information from free text represents a great solution, if done in the right manner. Previously, computer scientists broke a language into its grammatical forms, such as parts of speech, phrases, etc., using complex algorithms. Today, deep learning is a key to performing the same exercises.},
	pages = {1--74},
	booktitle = {Deep Learning for Natural Language Processing: Creating Neural Networks with Python},
	publisher = {Apress},
	author = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	editor = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	urldate = {2019-11-25},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-1-4842-3685-7_1},
	file = {Goyal et al. - Deep Learning for Natural Language Processing.pdf:/home/ritzo/Downloads/Telegram Desktop/Goyal et al. - Deep Learning for Natural Language Processing.pdf:application/pdf}
}

@online{noauthor_davidstutz/latex-resources_nodate,
	title = {davidstutz/latex-resources},
	url = {https://github.com/davidstutz/latex-resources},
	abstract = {Collection of {LaTeX} resources and examples. Contribute to davidstutz/latex-resources development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2019-11-25},
	langid = {english},
	file = {Snapshot:/home/ritzo/Zotero/storage/UIIRMVZ9/latex-resources.html:text/html}
}

@online{noauthor_martinthoma/latex-examples_nodate,
	title = {{MartinThoma}/{LaTeX}-examples},
	url = {https://github.com/MartinThoma/LaTeX-examples},
	abstract = {Examples for the usage of {LaTeX}. Contribute to {MartinThoma}/{LaTeX}-examples development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2019-11-25},
	langid = {english},
	file = {Snapshot:/home/ritzo/Zotero/storage/52232ZHT/LaTeX-examples.html:text/html}
}

@online{noauthor_dok_nodate,
	title = {Dok - Masterarbeit\_ZDFmediathek.pdf - Alle Dokumente},
	url = {https://accso.sharepoint.com/Tech/Forms/AllItems.aspx?FolderCTID=0x012000D1ECA4FD25E842439ED34EFBA3475DC5&id=%2FTech%2FHochschularbeit%2FAbschlussarbeiten%2FValentin%2DKuhn%2FMasterarbeit%5FZDFmediathek%2Epdf&parent=%2FTech%2FHochschularbeit%2FAbschlussarbeiten%2FValentin%2DKuhn},
	urldate = {2019-11-25},
	file = {Dok - Masterarbeit_ZDFmediathek.pdf - Alle Dokumente:/home/ritzo/Zotero/storage/DEMGPIHP/AllItems.html:text/html}
}

@article{kuhn_design_nodate,
	title = {Design and Implementation of a Recommender System for News in {ZDFmediathek} based on Deep Reinforcement Learning},
	pages = {62},
	author = {Kuhn, Valentin},
	langid = {german},
	file = {Masterarbeit_ZDFmediathek.pdf:C\:\\Users\\klein\\Downloads\\Masterarbeit_ZDFmediathek.pdf:application/pdf}
}

@article{kuhn_design_nodate-1,
	title = {Design and Implementation of a Recommender System for News in {ZDFmediathek} based on Deep Reinforcement Learning},
	pages = {62},
	author = {Kuhn, Valentin},
	langid = {german},
	file = {Masterarbeit_ZDFmediathek.pdf:/home/ritzo/Downloads/Masterarbeit_ZDFmediathek.pdf:application/pdf}
}

@article{zivkovic_efficient_2006,
	title = {Efficient adaptive density estimation per image pixel for the task of background subtraction},
	volume = {27},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865505003521},
	doi = {https://doi.org/10.1016/j.patrec.2005.11.005},
	abstract = {We analyze the computer vision task of pixel-level background subtraction. We present recursive equations that are used to constantly update the parameters of a Gaussian mixture model and to simultaneously select the appropriate number of components for each pixel. We also present a simple non-parametric adaptive density estimation method. The two methods are compared with each other and with some previously proposed algorithms.},
	pages = {773 -- 780},
	number = {7},
	journaltitle = {Pattern Recognition Letters},
	author = {Zivkovic, Zoran and Heijden, Ferdinand van der},
	date = {2006},
	keywords = {Background subtraction, Gaussian mixture model, Non-parametric density estimation, On-line density estimation}
}

@inproceedings{yu_xiaoyang_novel_2013,
	title = {A novel motion object detection method based on improved frame difference and improved Gaussian mixture model},
	volume = {01},
	doi = {10.1109/MIC.2013.6757972},
	pages = {309--313},
	booktitle = {Proceedings of 2013 2nd International Conference on Measurement, Information and Control},
	author = {{Yu Xiaoyang} and {Yu Yang} and {Yu Shuchun} and {Song Yang} and {Yang Huimin} and {Liu Xifeng}},
	date = {2013-08},
	keywords = {object detection, Gaussian mixture model, Adaptation models, background model, background substraction method, frame difference, frame difference improvement, Gaussian mixture model improvement, Gaussian processes, image motion analysis, motion detection, motion history image, motion object detection method, Motion segmentation, Robustness, spatio-temporal combination}
}

@article{mukherjee_adaptive_2013,
	title = {An Adaptive {GMM} Approach to Background Subtraction for Application in Real Time Surveillance},
	volume = {abs/1307.5800},
	url = {http://arxiv.org/abs/1307.5800},
	journaltitle = {{CoRR}},
	author = {Mukherjee, Subra and Das, Karen},
	date = {2013}
}

@inproceedings{peng_suo_improved_2008,
	title = {An improved adaptive background modeling algorithm based on Gaussian Mixture Model},
	doi = {10.1109/ICOSP.2008.4697402},
	pages = {1436--1439},
	booktitle = {2008 9th International Conference on Signal Processing},
	author = {{Peng Suo} and {Yanjiang Wang}},
	date = {2008-10},
	keywords = {Object detection, Gaussian mixture model, Gaussian processes, adaptive background modeling, Adaptive control, adaptive learning rate, background subtraction, Control engineering, Educational institutions, Electronic mail, Image motion analysis, image segmentation, Image segmentation, Layout, object segmentation, Petroleum, Programmable control, video sequence, video signal processing, video surveillance}
}

@inproceedings{mabrouk_performance_2019,
	location = {Singapore},
	title = {Performance and Scalability Improvement of {GMM} Background Segmentation Algorithm on Multi-core Parallel Platforms},
	isbn = {978-981-13-1405-6},
	abstract = {With the great evolution of multi-core platforms, the parallelization of many algorithms is the most efficient way for their real-time acceleration. Gaussian Mixture Model background subtraction ({GMM}) method is nowadays used in many moving object detection applications. This common approach is performed statistically on each single pixel in the captured frames. Thus, it is well suitable for parallel processing. In this paper, we propose some efficient tips to improve the performance and the scalability of {GMM} using the {OpenMP} framework. This is done by choosing a suitable structure to represent the background model as well as by merging computational loops to ensure contiguous memory access. The performance evaluation on a 16-cores Intel node showed that high performance and scalability are achieved even when high image resolutions are used.},
	pages = {120--127},
	booktitle = {Proceedings of the 1st International Conference on Electronic Engineering and Renewable Energy},
	publisher = {Springer Singapore},
	author = {Mabrouk, Lhoussein and Huet, Sylvain and Belkouch, Said and Houzet, Dominique and Zennayi, Yahya and Hamzaoui, Abdelkrim},
	editor = {Hajji, Bekkay and Tina, Giuseppe Marco and Ghoumid, Kamal and Rabhi, Abdelhamid and Mellit, Adel},
	date = {2019}
}

@inproceedings{nurhadiyatna_background_2013,
	title = {Background Subtraction Using Gaussian Mixture Model Enhanced by Hole Filling Algorithm ({GMMHF})},
	doi = {10.1109/SMC.2013.684},
	pages = {4006--4011},
	booktitle = {2013 {IEEE} International Conference on Systems, Man, and Cybernetics},
	author = {Nurhadiyatna, A. and Jatmiko, W. and Hardjono, B. and Wibisono, A. and Sina, I. and Mursanto, P.},
	date = {2013-10},
	keywords = {image classification, traffic engineering computing, Gaussian mixture model, Gaussian processes, background subtraction, video signal processing, Accuracy, Background Subtraction, camera, false classification, Filling, Gaussian Mixture Model, Gaussian mixture model enhanced by hole filling algorithm, {GMMHF}, Hafnium, Hole Filling Algorithm, illumination changes, Kappa statistic, Lighting, Noise, rippling water, road traffic control, traffic control system, video input, waving trees}
}

@inproceedings{braham_semantic_2017,
	location = {Beijing},
	title = {Semantic background subtraction},
	isbn = {978-1-5090-2175-8},
	url = {http://ieeexplore.ieee.org/document/8297144/},
	doi = {10.1109/ICIP.2017.8297144},
	abstract = {We introduce the notion of semantic background subtraction, a novel framework for motion detection in video sequences. The key innovation consists to leverage object-level semantics to address the variety of challenging scenarios for background subtraction. Our framework combines the information of a semantic segmentation algorithm, expressed by a probability for each pixel, with the output of any background subtraction algorithm to reduce false positive detections produced by illumination changes, dynamic backgrounds, strong shadows, and ghosts. In addition, it maintains a fully semantic background model to improve the detection of camouﬂaged foreground objects. Experiments led on the {CDNet} dataset show that we managed to improve, signiﬁcantly, almost all background subtraction algorithms of the {CDNet} leaderboard, and reduce the mean overall error rate of all the 34 algorithms (resp. of the best 5 algorithms) by roughly 50\% (resp. 20\%). Note that a C++ implementation of the framework is available at http://www.telecom.ulg.ac.be/semantic.},
	eventtitle = {2017 {IEEE} International Conference on Image Processing ({ICIP})},
	pages = {4552--4556},
	booktitle = {2017 {IEEE} International Conference on Image Processing ({ICIP})},
	publisher = {{IEEE}},
	author = {Braham, M. and Pierard, S. and Van Droogenbroeck, M.},
	urldate = {2019-11-27},
	date = {2017-09},
	langid = {english},
	file = {Braham et al. - 2017 - Semantic background subtraction.pdf:/home/ritzo/Zotero/storage/CS2FMEB9/Braham et al. - 2017 - Semantic background subtraction.pdf:application/pdf}
}

@article{bianco_combination_2017,
	title = {Combination of Video Change Detection Algorithms by Genetic Programming},
	volume = {21},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2017.2694160},
	pages = {914--928},
	number = {6},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	author = {Bianco, S. and Ciocca, G. and Schettini, R.},
	date = {2017-12},
	keywords = {Algorithm design and analysis, object detection, computer vision, Robustness, video signal processing, Algorithm combining and selection, binary functions, change detection, Change detection algorithms, {ChangeDetection}.net ({CDNET}), combination strategies, Detection algorithms, Evolutionary computation, Friedman test, genetic algorithms, genetic programming, Genetic programming, genetic programming ({GP}), ground truth annotations, image sequences, In Unity There Is Strength, n-ary functions, post-processing operations, statistical analysis, statistical significance analysis, Streaming media, unary functions, video change detection algorithms, video sequences, video stream, video streaming, Wilcoxon rank sum post-hoc tests}
}

@inproceedings{he_mask_2017,
	title = {Mask R-{CNN}},
	booktitle = {The {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	date = {2017-10},
	file = {He_Mask_R-CNN_ICCV_2017_paper.pdf:/home/ritzo/Zotero/storage/FHZ9X2ZM/He_Mask_R-CNN_ICCV_2017_paper.pdf:application/pdf}
}

@article{zheng_novel_2019,
	title = {A novel background subtraction algorithm based on parallel vision and Bayesian {GANs}},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219308811},
	doi = {https://doi.org/10.1016/j.neucom.2019.04.088},
	abstract = {To address the challenges of change detection in the wild, we present a novel background subtraction algorithm based on parallel vision and Bayesian generative adversarial networks ({GANs}). First, we use the median filtering algorithm for background image extraction. Then, we build the background subtraction model by using Bayesian {GANs} to classify all pixels into foreground and background, and use parallel vision theory to improve the background subtraction results in complex scenes. The proposed algorithm has been evaluated on the well-known, publicly available changedetection.net dataset. Experiment results show that the proposed algorithm results in better performance than many state-of-the-art ones. In addition, our model trained on {CDnet} dataset can generalize very well to unseen datasets, outperforming multiple state-of-art methods. The major contribution of this work is to apply parallel vision and Bayesian {GANs} to solve the difficulties in background subtraction, achieving high detection accuracy.},
	journaltitle = {Neurocomputing},
	author = {Zheng, Wenbo and Wang, Kunfeng and Wang, Fei-Yue},
	date = {2019},
	keywords = {Background subtraction, Background model, Bayesian generative adversarial network, Convolutional neural networks, Parallel vision}
}

@inproceedings{zheng_novel_2019-1,
	title = {A Novel Background Subtraction Algorithm Based on Parallel Vision and Bayesian {GANs}},
	author = {Zheng, Wenbo and Wang, Kunfeng and Wang, Fei-Yue},
	date = {2019}
}

@article{wang_interactive_2017,
	title = {Interactive deep learning method for segmenting moving objects},
	volume = {96},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865516302471},
	doi = {https://doi.org/10.1016/j.patrec.2016.09.014},
	abstract = {With the increasing number of machine learning methods used for segmenting images and analyzing videos, there has been a growing need for large datasets with pixel accurate ground truth. In this letter, we propose a highly accurate semi-automatic method for segmenting foreground moving objects pictured in surveillance videos. Given a limited number of user interventions, the goal of the method is to provide results sufficiently accurate to be used as ground truth. In this paper, we show that by manually outlining a small number of moving objects, we can get our model to learn the appearance of the background and the foreground moving objects. Since the background and foreground moving objects are highly redundant from one image to another (videos come from surveillance cameras) the model does not need a large number of examples to accurately fit the data. Our end-to-end model is based on a multi-resolution convolutional neural network ({CNN}) with a cascaded architecture. Tests performed on the largest publicly-available video dataset with pixel accurate groundtruth (changdetection.net) reveal that on videos from 11 categories, our approach has an average F-measure of 0.95 which is within the error margin of a human being. With our model, the amount of manual work for ground truthing a video gets reduced by a factor of up to 40. Code is made publicly available at: https://github.com/zhimingluo/{MovingObjectSegmentation}},
	pages = {66 -- 75},
	journaltitle = {Pattern Recognition Letters},
	author = {Wang, Yi and Luo, Zhiming and Jodoin, Pierre-Marc},
	date = {2017},
	keywords = {Convolutional neural network, Ground truthing, Motion detection}
}

@article{lim_foreground_2018,
	title = {Foreground segmentation using convolutional neural networks for multiscale feature encoding},
	volume = {112},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865518303702},
	doi = {https://doi.org/10.1016/j.patrec.2018.08.002},
	abstract = {Several methods have been proposed to solve moving objects segmentation problem accurately in different scenes. However, many of them lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose two robust encoder-decoder type neural networks that generate multi-scale feature encodings in different ways and can be trained end-to-end using only a few training samples. Using the same encoder-decoder configurations, in the first model, a triplet of encoders take the inputs in three scales to embed an image in a multi-scale feature space; in the second model, a Feature Pooling Module ({FPM}) is plugged on top of a single input encoder to extract multi-scale features in the middle layers. Both models use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. In order to evaluate our models, we entered the Change Detection 2014 Challenge (changedetection.net) and our models, namely {FgSegNet}\_M and {FgSegNet}\_S, outperformed all the existing state-of-the-art methods by an average F-Measure of 0.9770 and 0.9804, respectively. We also evaluate our models on {SBI}2015 and {UCSD} Background Subtraction datasets. Our source code is made publicly available at https://github.com/lim-anggun/{FgSegNet}.},
	pages = {256 -- 262},
	journaltitle = {Pattern Recognition Letters},
	author = {Lim, Long Ang and Keles, Hacer Yalim},
	date = {2018},
	keywords = {Deep learning, Background subtraction, Convolutional neural networks, Foreground segmentation, Pixel classification, Video surveillance}
}

@article{lim_learning_2019,
	title = {Learning Multi-scale Features for Foreground Segmentation},
	issn = {1433-7541, 1433-755X},
	url = {http://arxiv.org/abs/1808.01477},
	doi = {10.1007/s10044-019-00845-9},
	abstract = {Foreground segmentation algorithms aim segmenting moving objects from the background in a robust way under various challenging scenarios. Encoderdecoder type deep neural networks that are used in this domain recently perform impressive segmentation results. In this work, we propose a novel robust encoderdecoder structure neural network that can be trained end-to-end using only a few training examples. The proposed method extends the Feature Pooling Module ({FPM}) of {FgSegNet} by introducing features fusions inside this module, which is capable of extracting multiscale features within images; resulting in a robust feature pooling against camera motion, which can alleviate the need of multi-scale inputs to the network. Our method outperforms all existing state-of-the-art methods in {CDnet}2014 dataset by an average overall {FMeasure} of 0.9847. We also evaluate the eﬀectiveness of our method on {SBI}2015 and {UCSD} Background Subtraction datasets. The source code of the proposed method is made available at https://github.com/limanggun/{FgSegNet} v2.},
	journaltitle = {Pattern Analysis and Applications},
	shortjournal = {Pattern Anal Applic},
	author = {Lim, Long Ang and Keles, Hacer Yalim},
	urldate = {2019-11-27},
	date = {2019-08-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.01477},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Lim and Keles - 2019 - Learning Multi-scale Features for Foreground Segme.pdf:/home/ritzo/Zotero/storage/7V38MEBZ/Lim and Keles - 2019 - Learning Multi-scale Features for Foreground Segme.pdf:application/pdf}
}

@inproceedings{pham_gpu_2010,
	location = {Hanoi, Vietnam},
	title = {{GPU} Implementation of Extended Gaussian Mixture Model for Background Subtraction},
	isbn = {978-1-4244-8074-6},
	url = {http://ieeexplore.ieee.org/document/5634007/},
	doi = {10.1109/RIVF.2010.5634007},
	abstract = {Although trivial background subtraction ({BGS}) algorithms (e.g. frame differencing, running average...) can perform quite fast, they are not robust enough to be used in various computer vision problems. Some complex algorithms usually give better results, but are too slow to be applied to real-time systems. We propose an improved version of the Extended Gaussian mixture model that utilizes the computational power of Graphics Processing Units ({GPUs}) to achieve realtime performance. Experiments show that our implementation running on a low-end {GeForce} 9600GT {GPU} provides at least 10x speedup. The frame rate is greater than 50 frames per second (fps) for most of the tests, even on {HD} video formats.},
	eventtitle = {Communication Technologies, Research, Innovation, and Vision for the Future ({RIVF})},
	pages = {1--4},
	booktitle = {2010 {IEEE} {RIVF} International Conference on Computing \& Communication Technologies, Research, Innovation, and Vision for the Future ({RIVF})},
	publisher = {{IEEE}},
	author = {Pham, Vu and Vo, Phong and Hung, Vu Thanh and Bac, Le Hoai},
	urldate = {2019-11-27},
	date = {2010-11},
	langid = {english},
	file = {Pham et al. - 2010 - GPU Implementation of Extended Gaussian Mixture Mo.pdf:/home/ritzo/Zotero/storage/6FUX5IX6/Pham et al. - 2010 - GPU Implementation of Extended Gaussian Mixture Mo.pdf:application/pdf}
}

@inproceedings{zivkovic_improved_2004-1,
	location = {Cambridge, {UK}},
	title = {Improved adaptive Gaussian mixture model for background subtraction},
	isbn = {978-0-7695-2128-2},
	url = {http://ieeexplore.ieee.org/document/1333992/},
	doi = {10.1109/ICPR.2004.1333992},
	abstract = {Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We develop an efﬁcient adaptive algorithm using Gaussian mixture probability density. Recursive equations are used to constantly update the parameters and but also to simultaneously select the appropriate number of components for each pixel.},
	eventtitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. {ICPR} 2004.},
	pages = {28--31 Vol.2},
	booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. {ICPR} 2004.},
	publisher = {{IEEE}},
	author = {Zivkovic, Z.},
	urldate = {2019-11-27},
	date = {2004},
	langid = {english},
	file = {Zivkovic - 2004 - Improved adaptive Gaussian mixture model for backg.pdf:/home/ritzo/Zotero/storage/5DWPBRBW/Zivkovic - 2004 - Improved adaptive Gaussian mixture model for backg.pdf:application/pdf}
}

@book{szeliski_computer_2010,
	location = {Berlin, Heidelberg},
	edition = {1st},
	title = {Computer Vision: Algorithms and Applications},
	isbn = {1-84882-934-5 978-1-84882-934-3},
	publisher = {Springer-Verlag},
	author = {Szeliski, Richard},
	date = {2010}
}

@article{redmon_yolov3:_2018,
	title = {{YOLOv}3: An Incremental Improvement},
	volume = {abs/1804.02767},
	url = {http://arxiv.org/abs/1804.02767},
	journaltitle = {{CoRR}},
	author = {Redmon, Joseph and Farhadi, Ali},
	date = {2018},
	file = {Redmon und Farhadi - YOLOv3 An Incremental Improvement.pdf:/home/ritzo/Zotero/storage/LJ3VK9XD/Redmon und Farhadi - YOLOv3 An Incremental Improvement.pdf:application/pdf}
}

@book{nielsen_neural_2018,
	title = {Neural Networks and Deep Learning},
	url = {http://neuralnetworksanddeeplearning.com/},
	publisher = {Determination Press},
	author = {Nielsen, Michael A.},
	date = {2018},
	keywords = {ba-2018-hahnrico}
}

@article{goyal_review_2018,
	title = {Review of background subtraction methods using Gaussian mixture model for video surveillance systems},
	volume = {50},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-017-9542-x},
	doi = {10.1007/s10462-017-9542-x},
	abstract = {Foreground detection or moving object detection is a fundamental and critical task in video surveillance systems. Background subtraction using Gaussian Mixture Model ({GMM}) is a widely used approach for foreground detection. Many improvements have been proposed over the original {GMM} developed by Stauffer and Grimson ({IEEE} Computer Society conference on computer vision and pattern recognition, vol 2, Los Alamitos, pp 246–252, 1999. doi:10.1109/{CVPR}.1999.784637) to accommodate various challenges experienced in video surveillance systems. This paper presents a review of various background subtraction algorithms based on {GMM} and compares them on the basis of quantitative evaluation metrics. Their performance analysis is also presented to determine the most appropriate background subtraction algorithm for the speciﬁc application or scenario of video surveillance systems.},
	pages = {241--259},
	number = {2},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Goyal, Kalpana and Singhai, Jyoti},
	urldate = {2019-11-28},
	date = {2018-08},
	langid = {english},
	file = {goyal2017.pdf:/home/ritzo/Downloads/goyal2017.pdf:application/pdf}
}

@article{laugraud_labgen:_2017,
	title = {{LaBGen}: A method based on motion detection for generating the background of a scene},
	volume = {96},
	doi = {10.1016/j.patrec.2016.11.022},
	pages = {12--21},
	journaltitle = {Pattern Recognition Letters},
	author = {Laugraud, B. and Piérard, S. and Droogenbroeck, M. Van},
	date = {2017}
}

@inproceedings{stauffer_adaptive_1999,
	title = {Adaptive background mixture models for real-time tracking},
	volume = {2},
	doi = {10.1109/CVPR.1999.784637},
	pages = {246--252 Vol. 2},
	booktitle = {Proceedings. 1999 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No {PR}00149)},
	author = {Stauffer, C. and Grimson, W. E. L.},
	date = {1999-06},
	keywords = {Robustness, background subtraction, image segmentation, Image segmentation, Layout, image sequences, adaptive background mixture models, Adaptive systems, Artificial intelligence, Gaussian distribution, Image sequences, Laboratories, real-time segmentation, real-time systems, real-time tracking, thresholding, tracking, Tracking, Vehicle detection}
}

@article{chan_generalized_2011,
	title = {Generalized Stauffer–Grimson background subtraction for dynamic scenes},
	volume = {22},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-010-0262-3},
	doi = {10.1007/s00138-010-0262-3},
	abstract = {We propose an adaptive model for backgrounds containing significant stochastic motion (e.g. water). The new model is based on a generalization of the Stauffer–Grimson background model, where each mixture component is modeled as a dynamic texture. We derive an online K-means algorithm for updating the parameters using a set of sufficient statistics of the model. Finally, we report on experimental results, which show that the proposed background model both quantitatively and qualitatively outperforms state-of-the-art methods in scenes containing significant background motions.},
	pages = {751--766},
	number = {5},
	journaltitle = {Machine Vision and Applications},
	author = {Chan, Antoni B. and Mahadevan, Vijay and Vasconcelos, Nuno},
	date = {2011-09}
}

@article{shah_video_2014,
	title = {Video background modeling: recent approaches, issues and our proposed techniques},
	volume = {25},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-013-0552-7},
	doi = {10.1007/s00138-013-0552-7},
	shorttitle = {Video background modeling},
	abstract = {Effective and efficient background subtraction is important to a number of computer vision tasks. We introduce several new techniques to address key challenges for background modeling using a Gaussian mixture model ({GMM}) for moving objects detection in a video acquired by a static camera. The novel features of our proposed model are that it automatically learns dynamics of a scene and adapts its parameters accordingly, suppresses ghosts in the foreground mask using a {SURF} features matching algorithm, and introduces a new spatio-temporal filter to further refine the foreground detection results. Detection of abrupt illumination changes in the scene is dealt with by a model shifting-based scheme to reuse already learned models and spatio-temporal history of foreground blobs is used to detect and handle paused objects. The proposed model is rigorously tested and compared with several previous models and has shown significant performance improvements.},
	pages = {1105--1119},
	number = {5},
	journaltitle = {Machine Vision and Applications},
	shortjournal = {Machine Vision and Applications},
	author = {Shah, Munir and Deng, Jeremiah D. and Woodford, Brendon J.},
	urldate = {2019-11-28},
	date = {2014-07-01},
	langid = {english},
	keywords = {Background subtraction, Gaussian mixture model, Feature matching, Online learning}
}

@article{shah_video_2014-1,
	title = {Video background modeling: recent approaches, issues and our proposed techniques},
	volume = {25},
	issn = {1432-1769},
	url = {https://doi.org/10.1007/s00138-013-0552-7},
	doi = {10.1007/s00138-013-0552-7},
	abstract = {Effective and efficient background subtraction is important to a number of computer vision tasks. We introduce several new techniques to address key challenges for background modeling using a Gaussian mixture model ({GMM}) for moving objects detection in a video acquired by a static camera. The novel features of our proposed model are that it automatically learns dynamics of a scene and adapts its parameters accordingly, suppresses ghosts in the foreground mask using a {SURF} features matching algorithm, and introduces a new spatio-temporal filter to further refine the foreground detection results. Detection of abrupt illumination changes in the scene is dealt with by a model shifting-based scheme to reuse already learned models and spatio-temporal history of foreground blobs is used to detect and handle paused objects. The proposed model is rigorously tested and compared with several previous models and has shown significant performance improvements.},
	pages = {1105--1119},
	number = {5},
	journaltitle = {Machine Vision and Applications},
	author = {Shah, Munir and Deng, Jeremiah D. and Woodford, Brendon J.},
	date = {2014-07}
}

@inproceedings{bouwmans_background_2008,
	title = {Background Modeling using Mixture of Gaussians for Foreground Detection - A Survey},
	author = {Bouwmans, Thierry and Baf, Fida El and Vachon, Bertrand},
	date = {2008}
}

@article{lissner_toward_2012,
	title = {Toward a Unified Color Space for Perception-Based Image Processing},
	volume = {21},
	issn = {1941-0042},
	doi = {10.1109/TIP.2011.2163522},
	pages = {1153--1168},
	number = {3},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Lissner, I. and Urban, P.},
	date = {2012-03},
	keywords = {Noise, {CIE}94 color-difference formulas, {CIEDE}2000 color-difference formulas, {CMC} color-difference formulas, color attributes, color lookup tables, Color space, color-difference formulas, color-space transformations, hue linearity, human visual system, Humans, Image color analysis, image colour analysis, {MATLAB} codes, multigrid optimization, Noise reduction, optimisation, perception-based image processing, perceptual uniformity, Printers, unified color space, visual perception, Visual system}
}

@article{cucchiara_detecting_2003,
	title = {Detecting moving objects, ghosts, and shadows in video streams},
	volume = {25},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2003.1233909},
	pages = {1337--1342},
	number = {10},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cucchiara, R. and Grana, C. and Piccardi, M. and Prati, A.},
	date = {2003-10},
	keywords = {Object detection, object detection, image motion analysis, image segmentation, Layout, object segmentation, video surveillance, Streaming media, Video surveillance, apparent objects, Application software, background modeling, background subtraction methods, background update, color information, color segmentation, ghosts, human motion capture, Monitoring, moving object detection, Object segmentation, object-based selective update, object-level knowledge, pixel processing, shadow detection, Shape, statistical assumptions, Traffic control, traffic monitoring, {US} Department of Transportation, video streams}
}

@online{noauthor_definition_nodate,
	title = {Definition - Hysterese (Elektrotechnik) - item Glossar},
	url = {https://glossar.item24.com/glossarindex/artikel/item/hysterese-elektrotechnik.html},
	urldate = {2019-11-29},
	file = {Definition - Hysterese (Elektrotechnik) - item Glossar:/home/ritzo/Zotero/storage/C55GSTGA/hysterese-elektrotechnik.html:text/html}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	pages = {211--252},
	number = {3},
	journaltitle = {International Journal of Computer Vision ({IJCV})},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	date = {2015}
}

@article{lin_microsoft_2014,
	title = {Microsoft {COCO}: Common Objects in Context},
	volume = {abs/1405.0312},
	url = {http://arxiv.org/abs/1405.0312},
	journaltitle = {{CoRR}},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J. and Bourdev, Lubomir D. and Girshick, Ross B. and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	date = {2014}
}

@article{oshea_introduction_2015,
	title = {An Introduction to Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1511.08458},
	abstract = {The ﬁeld of machine learning has taken a dramatic twist in recent times, with the rise of the Artiﬁcial Neural Network ({ANN}). These biologically inspired computational models are able to far exceed the performance of previous forms of artiﬁcial intelligence in common machine learning tasks. One of the most impressive forms of {ANN} architecture is that of the Convolutional Neural Network ({CNN}). {CNNs} are primarily used to solve difﬁcult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simpliﬁed method of getting started with {ANNs}.},
	journaltitle = {{arXiv}:1511.08458 [cs]},
	author = {O'Shea, Keiron and Nash, Ryan},
	urldate = {2019-12-02},
	date = {2015-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.08458},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {1511.08458.pdf:/home/ritzo/Downloads/1511.08458.pdf:application/pdf}
}

@software{velickovic_petarv-/tikz_2019,
	title = {{PetarV}-/{TikZ}},
	rights = {{MIT}},
	url = {https://github.com/PetarV-/TikZ},
	abstract = {Complete collection of my {PGF}/{TikZ} figures. Contribute to {PetarV}-/{TikZ} development by creating an account on {GitHub}.},
	author = {Veličković, Petar},
	urldate = {2019-12-02},
	date = {2019-11-28},
	note = {original-date: 2016-07-18T16:24:31Z},
	keywords = {graphics, latex, pgf, tikz, tikz-figures}
}

@article{sobel_isotropic_2014,
	title = {An Isotropic 3x3 Image Gradient Operator},
	journaltitle = {Presentation at Stanford A.I. Project 1968},
	author = {Sobel, Irwin},
	date = {2014}
}

@inreference{wikipedia_hysterese_2019,
	title = {Hysterese},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://de.wikipedia.org/w/index.php?title=Hysterese&oldid=192979260},
	abstract = {Hysterese, auch Hysteresis („Nachwirkung“; griech. hysteros (ὕστερος) „hinterher, später“), ist eine Änderung der Wirkung, die verzögert gegenüber einer Änderung der Ursache auftritt (z. B. bei der thermostatgesteuerten Heizung die Differenz von Ein- und Ausschalttemperatur). Hysterese charakterisiert ein – bezogen auf die Eingangsgröße (bei der Heizung die Soll-Temperatur) – variant verzögertes Verhalten der bewirkten Ausgangsgröße (bei der Heizung die Ist-Temperatur), welche ihr Maximum bzw. ihr Minimum erreicht hat.
Allgemein formuliert handelt es sich bei Hysterese um ein Systemverhalten, bei dem die Ausgangsgröße nicht allein von der unabhängig veränderlichen Eingangsgröße, sondern auch vom vorherigen Zustand der Ausgangsgröße abhängt. Das System kann also – abhängig von der Vorgeschichte – bei gleicher Eingangsgröße einen von mehreren möglichen Zuständen einnehmen. Dieses Verhalten wird auch Pfadabhängigkeit genannt.
Hysterese tritt bei vielen natürlichen und technischen Vorgängen auf, insbesondere bei der Magnetisierung eines Magneten, in der Regelungstechnik und der Kybernetik. 
Typisch für Hystereseverhalten ist das Auftreten einer Hystereseschleife, die entsteht, indem man die verursachende Größe zwischen zwei verschiedenen Werten hin und her bewegt. Das bekannteste Phänomen ist das Hystereseverhalten eines Ferromagneten in einem Magnetfeld: Wird ein nicht magnetisierter Ferromagnet einem externen Feld ausgesetzt und dieses danach ausgeschaltet, so behält der Ferromagnet je nach Polung (d. h. Richtung) des externen Feldes eine positive oder negative Magnetisierung. Diese Restmagnetisierung wird als Remanenz bezeichnet.},
	booktitle = {Wikipedia},
	author = {Wikipedia},
	urldate = {2019-12-02},
	date = {2019-10-09},
	langid = {german},
	note = {Page Version {ID}: 192979260},
	file = {Snapshot:/home/ritzo/Zotero/storage/3QT9PNJI/index.html:text/html}
}

@online{brilliant_convolutional_nodate,
	title = {Convolutional Neural Network {\textbar} Brilliant Math \& Science Wiki},
	url = {https://brilliant.org/wiki/convolutional-neural-network/},
	abstract = {Convolutional neural networks (convnets, {CNNs}) are a powerful type of neural network that is used primarily for image classification. {CNNs} were originally designed by Geoffery Hinton, one of the pioneers of Machine Learning. Their location invariance makes them ideal for detecting objects in various positions in images. Google, Facebook, Snapchat and other companies that deal with images all use convolutional neural networks. Convnets consist primarily of three different types of layers: convolutions, pooling layers, and ...},
	author = {Brilliant},
	urldate = {2019-12-02},
	langid = {english},
	file = {Snapshot:/home/ritzo/Zotero/storage/L27XBR2G/convolutional-neural-network.html:text/html}
}

@online{singh_optimization_2019,
	title = {Optimization Algorithms in Deep Learning - Ashwin Singh - Medium},
	url = {https://medium.com/@ashwin8april/optimization-algorithms-in-deep-learning-4f2c3b53f9f},
	author = {Singh, Ashwin},
	urldate = {2019-12-02},
	date = {2019-01-09},
	file = {Optimization Algorithms in Deep Learning - Ashwin Singh - Medium:/home/ritzo/Zotero/storage/D5NJ35BQ/optimization-algorithms-in-deep-learning-4f2c3b53f9f.html:text/html}
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and Understanding Convolutional Networks},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the {ImageNet} benchmark (Krizhevsky et al., 2012). However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the {ImageNet} classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our {ImageNet} model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journaltitle = {{arXiv}:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	urldate = {2019-12-03},
	date = {2013-11-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:/home/ritzo/Zotero/storage/F9EJLFMW/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf}
}

@online{ng_maschinelles_nodate,
	title = {Maschinelles Lernen},
	url = {https://de.coursera.org/learn/machine-learning},
	abstract = {Learn Maschinelles Lernen from Stanford University. Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech ...},
	titleaddon = {Coursera},
	author = {Ng, Andrew},
	urldate = {2019-12-03},
	langid = {german},
	file = {Snapshot:/home/ritzo/Zotero/storage/TU5C5Z6T/machine-learning.html:text/html}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick and {others}},
	date = {1998}
}

@inproceedings{silva_investigation_2017,
	location = {Arequipa},
	title = {An investigation on the use of convolutional neural network for image classification in embedded systems},
	isbn = {978-1-5386-3734-0},
	url = {http://ieeexplore.ieee.org/document/8285727/},
	doi = {10.1109/LA-CCI.2017.8285727},
	abstract = {The study of Convolutional Neural Network ({CNN}) for image classiﬁcation is basically carried out on high performance and parallel platforms, so that the results of the literature cannot be replied on embedded systems. The aim of our work is to investigate {CNN} architectures that can run in such limited platforms and still maintain or improve the results of the current approaches. To that end, we specify and evaluate the performance of several {CNN} frameworks using different network conﬁgurations and dataset pre-processing techniques. The results of our ﬁnal approach show that its classiﬁcation efﬁciency is close to the best results of the literature, however using a much lower computational power.},
	eventtitle = {2017 {IEEE} Latin American Conference on Computational Intelligence ({LA}-{CCI})},
	pages = {1--6},
	booktitle = {2017 {IEEE} Latin American Conference on Computational Intelligence ({LA}-{CCI})},
	publisher = {{IEEE}},
	author = {Silva, Cecilia F. and Siebra, Clauirton A.},
	urldate = {2019-12-04},
	date = {2017-11},
	langid = {english},
	file = {08285727.pdf:/home/ritzo/Downloads/08285727.pdf:application/pdf}
}

@article{tan_mnasnet:_2019,
	title = {{MnasNet}: Platform-Aware Neural Architecture Search for Mobile},
	url = {http://arxiv.org/abs/1807.11626},
	shorttitle = {{MnasNet}},
	abstract = {Designing convolutional neural networks ({CNN}) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile {CNNs} on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search ({MNAS}) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., {FLOPS}), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile {CNN} models across multiple vision tasks. On the {ImageNet} classification task, our {MnasNet} achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than {MobileNetV}2 [29] with 0.5\% higher accuracy and 2.3x faster than {NASNet} [36] with 1.2\% higher accuracy. Our {MnasNet} also achieves better {mAP} quality than {MobileNets} for {COCO} object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet},
	journaltitle = {{arXiv}:1807.11626 [cs]},
	author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
	urldate = {2019-12-04},
	date = {2019-05-28},
	eprinttype = {arxiv},
	eprint = {1807.11626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/HAYPMX68/Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/FVKJATCH/1807.html:text/html}
}

@software{roeder_lutzroeder/netron_2019,
	title = {lutzroeder/netron},
	rights = {{MIT}},
	url = {https://github.com/lutzroeder/netron},
	abstract = {Visualizer for neural network, deep learning and machine learning models},
	author = {Roeder, Lutz},
	urldate = {2019-12-09},
	date = {2019-12-09},
	note = {original-date: 2010-12-26T12:53:43Z},
	keywords = {ai, caffe, caffe2, coreml, deep-learning, deeplearning, keras, machine-learning, machinelearning, ml, mxnet, neural-network, onnx, paddle, pytorch, scikit-learn, tensorflow, tensorflow-lite, torch, visualizer}
}

@article{kern_evaluierung_nodate,
	title = {Evaluierung der Qualität von Open Source Stream Processing Frameworks},
	pages = {160},
	author = {Kern, Simon},
	langid = {german},
	file = {00_OpenSourceStream.pdf:/home/ritzo/Downloads/00_OpenSourceStream.pdf:application/pdf}
}

@article{schuster_anforderungen_2019,
	title = {Anforderungen an eine fachlich valide Erprobung von technischen Systemen zur bedarfsgerechten Betriebsregulierung von Windenergieanlagen},
	journaltitle = {{KNE}, Kompetenzzentrum Naturschutz und Energiewende},
	author = {Schuster, Eva and Bruns, Elke},
	date = {2019-04},
	file = {KNE-Anforderungsprofil_an_eine_valide_Erprobung_von_technischen_Systemen_2019.pdf:/home/ritzo/Downloads/KNE-Anforderungsprofil_an_eine_valide_Erprobung_von_technischen_Systemen_2019.pdf:application/pdf}
}

@jurisdiction{noauthor_bnatschg_2009,
	title = {{BNatSchG} - Gesetz über Naturschutz und Landschaftspflege},
	url = {https://www.gesetze-im-internet.de/bnatschg_2009/BJNR254210009.html},
	urldate = {2019-12-17},
	date = {2009-07-29},
	file = {BNatSchG - Gesetz über Naturschutz und Landschaftspflege:/home/ritzo/Zotero/storage/N8W3WKMT/BJNR254210009.html:text/html}
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay},
	url = {http://arxiv.org/abs/1803.09820},
	shorttitle = {A disciplined approach to neural network hyper-parameters},
	abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efﬁcient ways to set the hyper-parameters that signiﬁcantly reduce training time and improves performance. Speciﬁcally, this report shows how to examine the training validation/test loss function for subtle clues of underﬁtting and overﬁtting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/{hyperParam}1.},
	journaltitle = {{arXiv}:1803.09820 [cs, stat]},
	author = {Smith, Leslie N.},
	urldate = {2019-12-18},
	date = {2018-04-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.09820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {1803.09820.pdf:/home/ritzo/Downloads/1803.09820.pdf:application/pdf}
}

@article{smith_cyclical_2017,
	title = {Cyclical Learning Rates for Training Neural Networks},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the {CIFAR}-10 and {CIFAR}-100 datasets with {ResNets}, Stochastic Depth networks, and {DenseNets}, and the {ImageNet} dataset with the {AlexNet} and {GoogLeNet} architectures. These are practical tools for everyone who trains neural networks.},
	journaltitle = {{arXiv}:1506.01186 [cs]},
	author = {Smith, Leslie N.},
	urldate = {2019-12-18},
	date = {2017-04-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.01186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {1506.01186.pdf:/home/ritzo/Downloads/1506.01186.pdf:application/pdf}
}

@online{noauthor_keras-team/keras-contrib_nodate,
	title = {keras-team/keras-contrib},
	url = {https://github.com/keras-team/keras-contrib},
	abstract = {Keras community contributions. Contribute to keras-team/keras-contrib development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2019-12-18},
	langid = {english},
	file = {Snapshot:/home/ritzo/Zotero/storage/9UIVSFTJ/cyclical_learning_rate.html:text/html}
}

@software{kenstler_bckenstler/clr_2019,
	title = {bckenstler/{CLR}},
	rights = {{MIT}},
	url = {https://github.com/bckenstler/CLR},
	abstract = {Contribute to bckenstler/{CLR} development by creating an account on {GitHub}.},
	author = {Kenstler, Brad},
	urldate = {2019-12-18},
	date = {2019-12-18},
	note = {original-date: 2017-03-21T21:48:47Z}
}

@article{han_dsd:_2016,
	title = {{DSD}: Regularizing Deep Neural Networks with Dense-Sparse-Dense Training Flow},
	volume = {abs/1607.04381},
	url = {http://arxiv.org/abs/1607.04381},
	journaltitle = {{CoRR}},
	author = {Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Tang, Shijian and Elsen, Erich and Catanzaro, Bryan and Tran, John and Dally, William J.},
	date = {2016}
}

@article{tan_efficientnet:_2019,
	title = {{EfficientNet}: Rethinking Model Scaling for Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1905.11946},
	shorttitle = {{EfficientNet}},
	abstract = {Convolutional Neural Networks ({ConvNets}) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up {MobileNets} and {ResNet}. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called {EfficientNets}, which achieve much better accuracy and efficiency than previous {ConvNets}. In particular, our {EfficientNet}-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on {ImageNet}, while being 8.4x smaller and 6.1x faster on inference than the best existing {ConvNet}. Our {EfficientNets} also transfer well and achieve state-of-the-art accuracy on {CIFAR}-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	journaltitle = {{arXiv}:1905.11946 [cs, stat]},
	author = {Tan, Mingxing and Le, Quoc V.},
	urldate = {2019-12-18},
	date = {2019-11-22},
	eprinttype = {arxiv},
	eprint = {1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/UW7NFCWU/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/XQR465BP/1905.html:text/html}
}

@article{szegedy_going_2014,
	title = {Going Deeper with Convolutions},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the {ImageNet} Large-Scale Visual Recognition Challenge 2014 ({ILSVRC} 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for {ILSVRC} 2014 is called {GoogLeNet}, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	journaltitle = {{arXiv}:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	urldate = {2019-12-20},
	date = {2014-09-16},
	eprinttype = {arxiv},
	eprint = {1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/2WG889YX/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/7VNKGTJD/1409.html:text/html}
}

@article{hu_squeeze-and-excitation_nodate,
	title = {Squeeze-and-Excitation Networks},
	abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive ﬁelds. In order to boost the representational power of a network, several recent approaches have shown the beneﬁt of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeezeand-Excitation” ({SE}) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct {SENet} architectures that generalise extremely well across challenging datasets. Crucially, we ﬁnd that {SE} blocks produce signiﬁcant performance improvements for existing state-ofthe-art deep architectures at minimal additional computational cost. {SENets} formed the foundation of our {ILSVRC} 2017 classiﬁcation submission which won ﬁrst place and signiﬁcantly reduced the top-5 error to 2.251\%, achieving a ∼25\% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/{SENet}.},
	pages = {10},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	langid = {english}
}

@article{huang_gpipe:_2019,
	title = {{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
	url = {http://arxiv.org/abs/1811.06965},
	shorttitle = {{GPipe}},
	abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce {GPipe}, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, {GPipe} provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, {GPipe} utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of {GPipe} by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter {AmoebaNet} model and attain a top-1 accuracy of 84.4\% on {ImageNet}-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
	journaltitle = {{arXiv}:1811.06965 [cs]},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, {HyoukJoong} and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	urldate = {2019-12-20},
	date = {2019-07-25},
	eprinttype = {arxiv},
	eprint = {1811.06965},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/YTHBWR9R/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/DHRRTEBY/1811.html:text/html}
}

@inproceedings{hu_squeeze-and-excitation_2018,
	title = {Squeeze-and-Excitation Networks},
	booktitle = {The {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	date = {2018-06},
	file = {Hu et al. - Squeeze-and-Excitation Networks.pdf:/home/ritzo/Zotero/storage/FXZVJYXH/Hu et al. - Squeeze-and-Excitation Networks.pdf:application/pdf}
}

@article{iandola_squeezenet:_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.5MB model size},
	url = {http://arxiv.org/abs/1602.07360},
	shorttitle = {{SqueezeNet}},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple {DNN} architectures that achieve that accuracy level. With equivalent accuracy, smaller {DNN} architectures offer at least three advantages: (1) Smaller {DNNs} require less communication across servers during distributed training. (2) Smaller {DNNs} require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller {DNNs} are more feasible to deploy on {FPGAs} and other hardware with limited memory. To provide all of these advantages, we propose a small {DNN} architecture called {SqueezeNet}. {SqueezeNet} achieves {AlexNet}-level accuracy on {ImageNet} with 50x fewer parameters. Additionally, with model compression techniques we are able to compress {SqueezeNet} to less than 0.5MB (510x smaller than {AlexNet}). The {SqueezeNet} architecture is available for download here: https://github.com/{DeepScale}/{SqueezeNet}},
	journaltitle = {{arXiv}:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	urldate = {2019-12-20},
	date = {2016-11-04},
	eprinttype = {arxiv},
	eprint = {1602.07360},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/M8BVXT7W/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/645MFWEA/1602.html:text/html}
}

@article{zhang_shufflenet:_2017,
	title = {{ShuffleNet}: An Extremely Efficient Convolutional Neural Network for Mobile Devices},
	url = {http://arxiv.org/abs/1707.01083},
	shorttitle = {{ShuffleNet}},
	abstract = {We introduce an extremely computation-efficient {CNN} architecture named {ShuffleNet}, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 {MFLOPs}). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on {ImageNet} classification and {MS} {COCO} object detection demonstrate the superior performance of {ShuffleNet} over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent {MobileNet} on {ImageNet} classification task, under the computation budget of 40 {MFLOPs}. On an {ARM}-based mobile device, {ShuffleNet} achieves {\textasciitilde}13x actual speedup over {AlexNet} while maintaining comparable accuracy.},
	journaltitle = {{arXiv}:1707.01083 [cs]},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	urldate = {2019-12-20},
	date = {2017-12-07},
	eprinttype = {arxiv},
	eprint = {1707.01083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/FDMUSS7W/Zhang et al. - 2017 - ShuffleNet An Extremely Efficient Convolutional N.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/K3FL3EB8/1707.html:text/html}
}

@article{ioffe_batch_2015,
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://arxiv.org/abs/1502.03167},
	shorttitle = {Batch Normalization},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	journaltitle = {{arXiv}:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	urldate = {2019-12-20},
	date = {2015-03-02},
	eprinttype = {arxiv},
	eprint = {1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/BCCXTA6Z/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/M83WWD3L/1502.html:text/html}
}

@article{xie_aggregated_2016-1,
	title = {Aggregated Residual Transformations for Deep Neural Networks},
	volume = {abs/1611.05431},
	url = {http://arxiv.org/abs/1611.05431},
	journaltitle = {{CoRR}},
	author = {Xie, Saining and Girshick, Ross B. and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	date = {2016}
}

@article{zoph_learning_2018,
	title = {Learning Transferable Architectures for Scalable Image Recognition},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "{NASNet} search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the {CIFAR}-10 dataset and then apply this cell to the {ImageNet} dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "{NASNet} architecture". We also introduce a new regularization technique called {ScheduledDropPath} that significantly improves generalization in the {NASNet} models. On {CIFAR}-10 itself, {NASNet} achieves 2.4\% error rate, which is state-of-the-art. On {ImageNet}, {NASNet} achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on {ImageNet}. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer {FLOPS} - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of {NASNets} exceed those of the state-of-the-art human-designed models. For instance, a small version of {NASNet} also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by {NASNet} used with the Faster-{RCNN} framework surpass state-of-the-art by 4.0\% achieving 43.1\% {mAP} on the {COCO} dataset.},
	journaltitle = {{arXiv}:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	urldate = {2019-12-20},
	date = {2018-04-11},
	eprinttype = {arxiv},
	eprint = {1707.07012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/EWXTFXKP/Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/TJ6KYTUH/1707.html:text/html}
}

@software{ujjwal_automold_2019,
	title = {Automold - Road Augmentation Library},
	rights = {{MIT}},
	url = {https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library},
	abstract = {This library augments road images to introduce various real world scenarios that pose challenges for training neural networks of Autonomous vehicles. Automold is created to train {CNNs} in specific w...},
	version = {4ba1753},
	author = {Ujjwal, Saxena},
	urldate = {2019-12-27},
	date = {2019-12-25},
	note = {original-date: 2018-04-08T11:28:29Z}
}

@article{tompson_efficient_2015,
	title = {Efficient Object Localization Using Convolutional Networks},
	url = {http://arxiv.org/abs/1411.4280},
	abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks ({ConvNets}). Traditional {ConvNet} architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art {ConvNet} model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the {FLIC} dataset and outperforms all existing approaches on the {MPII}-human-pose dataset.},
	journaltitle = {{arXiv}:1411.4280 [cs]},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and {LeCun}, Yann and Bregler, Christopher},
	urldate = {2019-12-27},
	date = {2015-06-09},
	eprinttype = {arxiv},
	eprint = {1411.4280},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/YNVEU5AS/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional .pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/YBJZK8UD/1411.html:text/html}
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2019-12-27},
	date = {2019-07-06},
	langid = {english},
	file = {Springer Full Text PDF:/home/ritzo/Zotero/storage/3D643CBZ/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf}
}

@software{yakubovskiy_qubvel/efficientnet_2019,
	title = {qubvel/efficientnet},
	rights = {Apache-2.0},
	url = {https://github.com/qubvel/efficientnet},
	abstract = {Implementation of {EfficientNet} model. Keras and {TensorFlow} Keras.},
	version = {aa1edca},
	author = {Yakubovskiy, Pavel},
	urldate = {2019-12-27},
	date = {2019-12-27},
	note = {original-date: 2019-05-30T20:21:09Z},
	keywords = {deep-learning, classification, efficient, efficientnet, image-classification, imagenet, mobilenet, nasnetmobile, pretrained-models}
}

@article{suzuki_topological_1985,
	title = {Topological structural analysis of digitized binary images by border following},
	volume = {30},
	issn = {0734-189X},
	url = {http://www.sciencedirect.com/science/article/pii/0734189X85900167},
	doi = {https://doi.org/10.1016/0734-189X(85)90016-7},
	abstract = {Two border following algorithms are proposed for the topological analysis of digitized binary images. The first one determines the surroundness relations among the borders of a binary image. Since the outer borders and the hole borders have a one-to-one correspondence to the connected components of 1-pixels and to the holes, respectively, the proposed algorithm yields a representation of a binary image, from which one can extract some sort of features without reconstructing the image. The second algorithm, which is a modified version of the first, follows only the outermost borders (i.e., the outer borders which are not surrounded by holes). These algorithms can be effectively used in component counting, shrinking, and topological structural analysis of binary images, when a sequential digital computer is used.},
	pages = {32 -- 46},
	number = {1},
	journaltitle = {Computer Vision, Graphics, and Image Processing},
	author = {Suzuki, Satoshi and Abe, Keiichi},
	date = {1985},
	file = {Topological_Structural_Analysis_of_Digit.pdf:/home/ritzo/Downloads/Topological_Structural_Analysis_of_Digit.pdf:application/pdf}
}

@inproceedings{smith_cyclical_2017-1,
	title = {Cyclical Learning Rates for Training Neural Networks},
	doi = {10.1109/WACV.2017.58},
	pages = {464--472},
	booktitle = {2017 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
	author = {Smith, L. N.},
	date = {2017-03},
	keywords = {image classification, neural nets, Neural networks, {AlexNet} architecture, boundary values, {CIFAR}-10 dataset, {CIFAR}-100 dataset, classification accuracy improvement, Computational efficiency, Computer architecture, cyclical learning rates, deep neural network training, {DenseNets}, global learning rates, {GoogLeNet} architecture, hyper-parameter, {ImageNet} dataset, learning (artificial intelligence), {ResNets} dataset, Schedules, stochastic depth networks, Training, Tuning, visual databases}
}

@article{raschka_mlxtend_2018,
	title = {{MLxtend}: Providing machine learning and data science utilities and extensions to Python’s scientific computing stack},
	volume = {3},
	url = {http://joss.theoj.org/papers/10.21105/joss.00638},
	doi = {10.21105/joss.00638},
	number = {24},
	journaltitle = {The Journal of Open Source Software},
	author = {Raschka, Sebastian},
	date = {2018-04}
}

@software{smith_imagenetscraper_2017,
	title = {imagenetscraper: Bulk-download thumbnails from {ImageNet} synsets},
	url = {https://github.com/spinda/imagenetscraper},
	author = {Smith, Michael},
	date = {2017}
}

@article{srivastava_dropout_nodate,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	pages = {30},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	langid = {english}
}

@article{srivastava_dropout_2014,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	volume = {15},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	pages = {1929--1958},
	journaltitle = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014},
	file = {srivastava14a.pdf:/home/ritzo/Downloads/srivastava14a.pdf:application/pdf}
}

@article{santurkar_how_2019,
	title = {How Does Batch Normalization Help Optimization?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization ({BatchNorm}) is a widely adopted technique that enables faster and more stable training of deep neural networks ({DNNs}). Despite its pervasiveness, the exact reasons for {BatchNorm}’s effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers’ input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of {BatchNorm}. Instead, we uncover a more fundamental impact of {BatchNorm} on the training process: it makes the optimization landscape signiﬁcantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	journaltitle = {{arXiv}:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	urldate = {2020-01-06},
	date = {2019-04-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.11604},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {1805.11604.pdf:/home/ritzo/Downloads/1805.11604.pdf:application/pdf}
}

@software{yakubovskiy_qubvelefficientnet_2020,
	title = {qubvel/efficientnet},
	rights = {Apache-2.0},
	url = {https://github.com/qubvel/efficientnet},
	abstract = {Implementation of {EfficientNet} model. Keras and {TensorFlow} Keras.},
	author = {Yakubovskiy, Pavel},
	urldate = {2020-01-07},
	date = {2020-01-06},
	note = {original-date: 2019-05-30T20:21:09Z},
	keywords = {deep-learning, classification, efficient, efficientnet, image-classification, imagenet, mobilenet, nasnetmobile, pretrained-models}
}

@book{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015}
}

@article{merkel_docker_2014,
	title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
	volume = {2014},
	issn = {1075-3583},
	number = {239},
	journaltitle = {Linux J.},
	author = {Merkel, Dirk},
	date = {2014-03}
}

@article{razakarivony_vehicle_2016-1,
	title = {Vehicle detection in aerial imagery : A small target detection benchmark},
	volume = {34},
	issn = {10473203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047320315002187},
	doi = {10.1016/j.jvcir.2015.11.002},
	shorttitle = {Vehicle detection in aerial imagery},
	abstract = {This paper introduces {VEDAI}: Vehicle Detection in Aerial Imagery a new database of aerial images provided as a tool to benchmark automatic target recognition algorithms in unconstrained environments. The vehicles contained in the database, in addition of being small, exhibit diﬀerent variabilities such as multiple orientations, lighting/shadowing changes, specularities or occlusions. Furthermore, each image is available in several spectral bands and resolutions. A precise experimental protocol is also given, ensuring that the experimental results obtained by diﬀerent people can be properly reproduce and compared. Finally, the paper also gives the performance of baseline algorithms on this dataset, for diﬀerent settings of these algorithms, to illustrate the diﬃculties of the task and provide baseline comparisons.},
	pages = {187--203},
	journaltitle = {Journal of Visual Communication and Image Representation},
	shortjournal = {Journal of Visual Communication and Image Representation},
	author = {Razakarivony, Sebastien and Jurie, Frederic},
	urldate = {2020-01-13},
	date = {2016-01},
	langid = {english},
	file = {document.pdf:/home/ritzo/Zotero/storage/37Y7USA5/document.pdf:application/pdf}
}

@inproceedings{piccardi_background_2004,
	location = {The Hague, Netherlands},
	title = {Background subtraction techniques: a review},
	isbn = {978-0-7803-8567-2},
	url = {http://ieeexplore.ieee.org/document/1400815/},
	doi = {10.1109/ICSMC.2004.1400815},
	shorttitle = {Background subtraction techniques},
	eventtitle = {2004 {IEEE} International Conference on Systems, Man and Cybernetics},
	pages = {3099--3104},
	booktitle = {2004 {IEEE} International Conference on Systems, Man and Cybernetics ({IEEE} Cat. No.04CH37583)},
	publisher = {{IEEE}},
	author = {Piccardi, M.},
	urldate = {2020-01-13},
	date = {2004},
	langid = {english},
	file = {Piccardi - 2004 - Background subtraction techniques a review.pdf:/home/ritzo/Zotero/storage/IR8MQFKA/Piccardi - 2004 - Background subtraction techniques a review.pdf:application/pdf}
}

@online{landesbund_fur_vogelschutz_in_bayern_rotmilan_nodate,
	title = {Rotmilan},
	url = {https://www.lbv.de/ratgeber/naturwissen/artenportraits/detail/rotmilan/},
	abstract = {Wie der Name schon sagt, hat der Rotmilan einen hell bräunlich bis rostroten Rücken. Der Vogel ist wegen seines tief gespaltenen Schwanzes auch unter dem Namen 'Gabelweihe' bekannt.},
	titleaddon = {lbv.de},
	author = {{Landesbund für Vogelschutz in Bayern}},
	urldate = {2020-01-13},
	langid = {german},
	file = {Snapshot:/home/ritzo/Zotero/storage/IBZ9AA4I/rotmilan.html:text/html}
}

@article{grunkorn_ermittlung_2016,
	title = {Ermittlung der Kollisionsraten von (Greif-)Vögeln und Schaffung planungsbezogener Grundlagen für die Prognose und Bewertung des Kollisionsrisikos durch Windenergieanlagen ({PROGRESS})},
	url = {http://rgdoi.net/10.13140/RG.2.1.2902.6800},
	doi = {10.13140/RG.2.1.2902.6800},
	author = {Grünkorn, Thomas and Rönn, Jan Von and Blew, Jan and Nehls, Georg and Weitekamp, Sabrina and Timmermann, Hanna and Reichenbach, Marc and Coppack, Timothy and Potiek, Astrid and Krüger, Oliver},
	urldate = {2020-01-13},
	date = {2016},
	langid = {german},
	file = {1561-1.pdf:/home/ritzo/Zotero/storage/B9PACW2I/1561-1.pdf:application/pdf}
}

@report{noauthor_ermittlung_nodate,
	title = {Ermittlung der Kollisionsraten von (Greif-)Vögeln und Schaffung planungsbezogener Grundlagen für die Prognose und Bewertung des Kollisionsrisikos durch Windenergieanlagen}
}

@book{kohler_windenergie_2016,
	title = {Windenergie und Rotmilan: ein Scheinproblem},
	publisher = {{KohleNusbaumer} {SA}},
	author = {Kohler, Oliver},
	date = {2016},
	file = {886e3c_a03b09fc4e46418c89295ec31209642d.pdf:/home/ritzo/Downloads/886e3c_a03b09fc4e46418c89295ec31209642d.pdf:application/pdf}
}

@book{suthaharan_machine_2016,
	location = {New York Heidelberg Dordrecht London},
	title = {Machine learning models and algorithms for big data classification: thinking with examples for effective learning},
	isbn = {978-1-4899-7640-6 978-1-4899-7641-3},
	series = {Integrated series in information systems},
	shorttitle = {Machine learning models and algorithms for big data classification},
	pagetotal = {359},
	number = {36},
	publisher = {Springer},
	author = {Suthaharan, Shan},
	date = {2016},
	note = {{OCLC}: 934649754}
}

@book{suthaharan_machine_2016-1,
	location = {New York Heidelberg Dordrecht London},
	title = {Machine learning models and algorithms for big data classification: thinking with examples for effective learning},
	isbn = {978-1-4899-7641-3 978-1-4899-7640-6},
	series = {Integrated series in information systems},
	shorttitle = {Machine learning models and algorithms for big data classification},
	pagetotal = {359},
	number = {36},
	publisher = {Springer},
	author = {Suthaharan, Shan},
	date = {2016},
	note = {{OCLC}: 934649754}
}

@article{li_visualizing_2018,
	title = {Visualizing the Loss Landscape of Neural Nets},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	journaltitle = {{arXiv}:1712.09913 [cs, stat]},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	urldate = {2020-01-16},
	date = {2018-11-07},
	eprinttype = {arxiv},
	eprint = {1712.09913},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/YNGDIL6N/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/VGGW7X8S/1712.html:text/html}
}

@article{saito_precision-recall_2015,
	title = {The Precision-Recall Plot Is More Informative than the {ROC} Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
	volume = {10},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0118432},
	doi = {10.1371/journal.pone.0118432},
	pages = {e0118432},
	number = {3},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Saito, Takaya and Rehmsmeier, Marc},
	editor = {Brock, Guy},
	urldate = {2020-01-21},
	date = {2015-03-04},
	langid = {english},
	file = {Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf:/home/ritzo/Zotero/storage/EVKCPNSF/Saito and Rehmsmeier - 2015 - The Precision-Recall Plot Is More Informative than.pdf:application/pdf}
}

@book{wada_labelme_2016,
	title = {labelme: Image Polygonal Annotation with Python},
	url = {https://github.com/wkentaro/labelme},
	author = {Wada, Ketaro},
	date = {2016}
}

@software{tzutalin_labelimg_2020,
	title = {{LabelImg}},
	rights = {{MIT}},
	url = {https://github.com/tzutalin/labelImg},
	abstract = {🖍️ {LabelImg} is a graphical image annotation tool and label object bounding boxes in images},
	author = {{Tzutalin}},
	urldate = {2020-01-22},
	date = {2020-01-22},
	note = {original-date: 2015-09-17T01:33:59Z},
	keywords = {deep-learning, image-classification, imagenet, annotations, detection, python2, python3, recognition, tools}
}

@inproceedings{wirth_crisp-dm_2000,
	title = {{CRISP}-{DM}: Towards a standard process model for data mining},
	pages = {29--39},
	booktitle = {Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining},
	publisher = {Springer-Verlag London, {UK}},
	author = {Wirth, Rüdiger and Hipp, Jochen},
	date = {2000},
	file = {293cfd4297f855867ca278f7069abc6a9c24.pdf:/home/ritzo/Downloads/293cfd4297f855867ca278f7069abc6a9c24.pdf:application/pdf}
}

@software{jung_imgaug_2019,
	title = {imgaug},
	url = {https://github.com/aleju/imgaug},
	author = {Jung, Alexander B. and Wada, Kentaro and Crall, Jon and Tanaka, Satoshi and Graving, Jake and Yadav, Sarthak and Banerjee, Joy and Vecsei, Gábor and Kraft, Adam and Borovec, Jirka and Vallentin, Christian and Zhydenko, Semen and Pfeiffer, Kilian and Cook, Ben and Fernández, Ismael and Chi-Hung, Weng and Ayala-Acevedo, Abner and Meudec, Raphael and Laporte, Matias and {others}},
	urldate = {2020-01-27},
	date = {2019}
}

@article{everingham_pascal_2015,
	title = {The Pascal Visual Object Classes Challenge: A Retrospective},
	volume = {111},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-014-0733-5},
	doi = {10.1007/s11263-014-0733-5},
	shorttitle = {The Pascal Visual Object Classes Challenge},
	pages = {98--136},
	number = {1},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Everingham, Mark and Eslami, S. M. Ali and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	urldate = {2020-01-30},
	date = {2015-01},
	langid = {english},
	file = {Everingham et al. - 2015 - The Pascal Visual Object Classes Challenge A Retr.pdf:/home/ritzo/Zotero/storage/WTG5PBDY/Everingham et al. - 2015 - The Pascal Visual Object Classes Challenge A Retr.pdf:application/pdf}
}

@article{tan_efficientdet_2019,
	title = {{EfficientDet}: Scalable and Efficient Object Detection},
	url = {http://arxiv.org/abs/1911.09070},
	shorttitle = {{EfficientDet}},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study various neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network ({BiFPN}), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations, we have developed a new family of object detectors, called {EfficientDet}, which consistently achieve an order-of-magnitude better efficiency than prior art across a wide spectrum of resource constraints. In particular, without bells and whistles, our {EfficientDet}-D7 achieves stateof-the-art 51.0 {mAP} on {COCO} dataset with 52M parameters and 326B {FLOPS}1 , being 4x smaller and using 9.3x fewer {FLOPS} yet still more accurate (+0.3\% {mAP}) than the best previous detector.},
	journaltitle = {{arXiv}:1911.09070 [cs, eess]},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	urldate = {2020-02-05},
	date = {2019-11-20},
	eprinttype = {arxiv},
	eprint = {1911.09070},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/ritzo/Zotero/storage/7C5LHKGK/Tan et al. - 2019 - EfficientDet Scalable and Efficient Object Detect.pdf:application/pdf;arXiv.org Snapshot:/home/ritzo/Zotero/storage/GDFLRWIQ/1911.html:text/html}
}

@online{lambda_labs_inc_gpu_nodate,
	title = {{GPU} Training Leaderboard},
	url = {https://lambdalabs.com/deep-learning/gpu-benchmarks},
	author = {{Lambda Labs, inc.}},
	urldate = {2020-02-05},
	file = {GPU Training Leaderboard:/home/ritzo/Zotero/storage/LLU2YNS7/gpu-benchmarks.html:text/html}
}

@online{noauthor_jetson_nodate,
	title = {Jetson - {eLinux}.org},
	url = {https://elinux.org/Jetson},
	urldate = {2020-02-05},
	file = {Jetson - eLinux.org:/home/ritzo/Zotero/storage/S3PMV9GG/Jetson.html:text/html}
}

@online{spence_xavier_nodate,
	title = {Xavier \& {TX}2 Comparison},
	url = {http://connecttech.com/xavier-tx2-comparison/},
	abstract = {What is {AGX} Xavier? 
The {NVIDIA}® Jetson™ {AGX} Xavier™ is the world’s first {AI} computer for autonomous machines. 

 	20x performance than Jetson™ {TX}2
 	512-core Volta {GPU} and 64 Tensor cores with discreet dual Deep Learning Accelerator ({DLA}) {NVDLA} engines
  

 	4 x dual-core {CPU} clusters (8 {NVIDIA}},
	titleaddon = {Connect Tech Inc.},
	author = {{spence}},
	urldate = {2020-02-05},
	langid = {american},
	file = {Snapshot:/home/ritzo/Zotero/storage/J3SEWIS6/xavier-tx2-comparison.html:text/html}
}

@online{nvidia_nvidia_nodate,
	title = {{NVIDIA} Embedded Systems for Next-Gen Autonomous Machines},
	url = {https://www.nvidia.com/de-de/autonomous-machines/embedded-systems/},
	abstract = {{NVIDIA} Jetson ist die weltweit führende {KI}-Computing-Plattform für grafikprozessorbeschleunigte parallele Datenverarbeitung in eingebetteten mobilen Systemen.},
	author = {{NVIDIA}},
	urldate = {2020-02-05},
	langid = {german},
	file = {Snapshot:/home/ritzo/Zotero/storage/TZSKRD86/embedded-systems.html:text/html}
}